{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "       load parameters from pre-train models.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "    \n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    \n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    \n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding='SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "    \n",
    "    \n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize = [1,2,2,1],\n",
    "                              strides = [1,2,2,1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name)\n",
    "    \n",
    "    def fc_layer(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "    \n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x\n",
    "    \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"Build VGG16 network structure.\n",
    "        Parameters:\n",
    "        - x_rgb: [1, 224, 224, 3]\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('building model ...')\n",
    "        \n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis=3)\n",
    "        x_bgr = tf.concat(\n",
    "            [b - VGG_MEAN[0],\n",
    "             g - VGG_MEAN[1],\n",
    "             r - VGG_MEAN[2]],\n",
    "            axis = 3)\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "\n",
    "#         self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "#         self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "#         self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "#         self.fc8 = self.fc_layer(self.fc7, 'fc8', activation=None)\n",
    "#         self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "  \n",
    "        \n",
    "        print ('building model finished: %4ds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16_npy_path = './vgg16.npy'\n",
    "# data_dict = np.load(vgg16_npy_path,allow_pickle=True, encoding='latin1').item()\n",
    "\n",
    "# vgg16_for_result = VGGNet(data_dict)\n",
    "# content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "# vgg16_for_result.build(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_npy_path = './vgg16.npy'\n",
    "content_img_path = './source_images/2.jpg'\n",
    "style_img_path = './source_images/1.jpg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = './run_style_transfer'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "building model ...\n",
      "building model finished:    0s\n",
      "building model ...\n",
      "building model finished:    0s\n",
      "building model ...\n",
      "building model finished:    0s\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "def initial_result(shape, mean, stddev):\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img) # (224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) # (1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calculates gram matrix\n",
    "    Args: \n",
    "    - x: features extractedd from VGG Net. shape[1, width, height, ch]\n",
    "    \"\"\"\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, h*w, ch]) # [ch, ch] -> (i, j)\n",
    "    # [h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch]\n",
    "    gram = tf.matmul(features, features, adjoint_a=True) / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "    \n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "# content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "# style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "data_dict = np.load(vgg16_npy_path,allow_pickle=True, encoding='latin1').item()\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "content_features = [\n",
    "            vgg_for_content.conv1_2,\n",
    "            vgg_for_content.conv2_2,\n",
    "#             vgg_for_content.conv3_3,\n",
    "#             vgg_for_content.conv4_3,\n",
    "#             vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "\n",
    "result_content_features = [\n",
    "            vgg_for_result.conv1_2,\n",
    "            vgg_for_result.conv2_2,\n",
    "#             vgg_for_result.conv3_3,\n",
    "#             vgg_for_result.conv4_3,\n",
    "#             vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# feature_size, [1, width, height, channel]\n",
    "style_features = [ \n",
    "#             vgg_for_style.conv1_2,\n",
    "#             vgg_for_style.conv2_2,\n",
    "#             vgg_for_style.conv3_3,\n",
    "            vgg_for_style.conv4_3,\n",
    "#             vgg_for_style.conv5_3\n",
    "]\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "#             vgg_for_style.conv1_2,\n",
    "#             vgg_for_style.conv2_2,\n",
    "#             vgg_for_style.conv3_3,\n",
    "            vgg_for_style.conv4_3,\n",
    "#             vgg_for_style.conv5_3\n",
    "]\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# zip: [1,2], [3,4], zip([1,2], [3,4]) -> [(1,3), (2,4)]\n",
    "# shape: [1, width, height, channel]\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])\n",
    "    \n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "    \n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 31185.5508, content_loss: 311855.5000, style_loss:   0.0000\n",
      "step: 2, loss_value: 24606.4648, content_loss: 246064.6406, style_loss:   0.0000\n",
      "step: 3, loss_value: 19823.6387, content_loss: 198236.3750, style_loss:   0.0000\n",
      "step: 4, loss_value: 16263.8438, content_loss: 162638.4375, style_loss:   0.0000\n",
      "step: 5, loss_value: 13517.8467, content_loss: 135178.4688, style_loss:   0.0000\n",
      "step: 6, loss_value: 11392.1289, content_loss: 113921.2891, style_loss:   0.0000\n",
      "step: 7, loss_value: 9795.4111, content_loss: 97954.1094, style_loss:   0.0000\n",
      "step: 8, loss_value: 8550.9297, content_loss: 85509.2969, style_loss:   0.0000\n",
      "step: 9, loss_value: 7544.6851, content_loss: 75446.8516, style_loss:   0.0000\n",
      "step: 10, loss_value: 6730.9360, content_loss: 67309.3594, style_loss:   0.0000\n",
      "step: 11, loss_value: 6068.1870, content_loss: 60681.8672, style_loss:   0.0000\n",
      "step: 12, loss_value: 5520.9761, content_loss: 55209.7578, style_loss:   0.0000\n",
      "step: 13, loss_value: 5070.8530, content_loss: 50708.5312, style_loss:   0.0000\n",
      "step: 14, loss_value: 4697.2002, content_loss: 46972.0000, style_loss:   0.0000\n",
      "step: 15, loss_value: 4372.5132, content_loss: 43725.1289, style_loss:   0.0000\n",
      "step: 16, loss_value: 4082.2500, content_loss: 40822.5000, style_loss:   0.0000\n",
      "step: 17, loss_value: 3811.7012, content_loss: 38117.0117, style_loss:   0.0000\n",
      "step: 18, loss_value: 3547.0872, content_loss: 35470.8711, style_loss:   0.0000\n",
      "step: 19, loss_value: 3288.5593, content_loss: 32885.5938, style_loss:   0.0000\n",
      "step: 20, loss_value: 3040.6738, content_loss: 30406.7383, style_loss:   0.0000\n",
      "step: 21, loss_value: 2807.7207, content_loss: 28077.2070, style_loss:   0.0000\n",
      "step: 22, loss_value: 2589.4648, content_loss: 25894.6484, style_loss:   0.0000\n",
      "step: 23, loss_value: 2386.3110, content_loss: 23863.1094, style_loss:   0.0000\n",
      "step: 24, loss_value: 2198.1008, content_loss: 21981.0078, style_loss:   0.0000\n",
      "step: 25, loss_value: 2027.1903, content_loss: 20271.9023, style_loss:   0.0000\n",
      "step: 26, loss_value: 1873.0267, content_loss: 18730.2676, style_loss:   0.0000\n",
      "step: 27, loss_value: 1732.7975, content_loss: 17327.9746, style_loss:   0.0000\n",
      "step: 28, loss_value: 1607.8192, content_loss: 16078.1924, style_loss:   0.0000\n",
      "step: 29, loss_value: 1499.0850, content_loss: 14990.8496, style_loss:   0.0000\n",
      "step: 30, loss_value: 1404.2577, content_loss: 14042.5762, style_loss:   0.0000\n",
      "step: 31, loss_value: 1320.4424, content_loss: 13204.4238, style_loss:   0.0000\n",
      "step: 32, loss_value: 1246.0270, content_loss: 12460.2695, style_loss:   0.0000\n",
      "step: 33, loss_value: 1178.5757, content_loss: 11785.7568, style_loss:   0.0000\n",
      "step: 34, loss_value: 1115.8296, content_loss: 11158.2959, style_loss:   0.0000\n",
      "step: 35, loss_value: 1056.5474, content_loss: 10565.4736, style_loss:   0.0000\n",
      "step: 36, loss_value: 1000.9482, content_loss: 10009.4814, style_loss:   0.0000\n",
      "step: 37, loss_value: 949.4494, content_loss: 9494.4941, style_loss:   0.0000\n",
      "step: 38, loss_value: 901.4179, content_loss: 9014.1787, style_loss:   0.0000\n",
      "step: 39, loss_value: 855.1619, content_loss: 8551.6191, style_loss:   0.0000\n",
      "step: 40, loss_value: 811.1609, content_loss: 8111.6089, style_loss:   0.0000\n",
      "step: 41, loss_value: 769.6010, content_loss: 7696.0093, style_loss:   0.0000\n",
      "step: 42, loss_value: 730.1427, content_loss: 7301.4268, style_loss:   0.0000\n",
      "step: 43, loss_value: 692.0232, content_loss: 6920.2319, style_loss:   0.0000\n",
      "step: 44, loss_value: 655.3849, content_loss: 6553.8496, style_loss:   0.0000\n",
      "step: 45, loss_value: 620.2288, content_loss: 6202.2881, style_loss:   0.0000\n",
      "step: 46, loss_value: 586.5789, content_loss: 5865.7891, style_loss:   0.0000\n",
      "step: 47, loss_value: 554.8442, content_loss: 5548.4414, style_loss:   0.0000\n",
      "step: 48, loss_value: 524.5193, content_loss: 5245.1934, style_loss:   0.0000\n",
      "step: 49, loss_value: 495.0222, content_loss: 4950.2217, style_loss:   0.0000\n",
      "step: 50, loss_value: 466.7903, content_loss: 4667.9023, style_loss:   0.0000\n",
      "step: 51, loss_value: 440.0950, content_loss: 4400.9502, style_loss:   0.0000\n",
      "step: 52, loss_value: 414.8724, content_loss: 4148.7241, style_loss:   0.0000\n",
      "step: 53, loss_value: 390.8561, content_loss: 3908.5610, style_loss:   0.0000\n",
      "step: 54, loss_value: 368.1923, content_loss: 3681.9229, style_loss:   0.0000\n",
      "step: 55, loss_value: 346.9553, content_loss: 3469.5530, style_loss:   0.0000\n",
      "step: 56, loss_value: 326.9171, content_loss: 3269.1704, style_loss:   0.0000\n",
      "step: 57, loss_value: 308.3230, content_loss: 3083.2295, style_loss:   0.0000\n",
      "step: 58, loss_value: 290.9482, content_loss: 2909.4824, style_loss:   0.0000\n",
      "step: 59, loss_value: 274.7869, content_loss: 2747.8691, style_loss:   0.0000\n",
      "step: 60, loss_value: 259.7970, content_loss: 2597.9702, style_loss:   0.0000\n",
      "step: 61, loss_value: 245.7502, content_loss: 2457.5015, style_loss:   0.0000\n",
      "step: 62, loss_value: 232.4078, content_loss: 2324.0776, style_loss:   0.0000\n",
      "step: 63, loss_value: 219.9239, content_loss: 2199.2385, style_loss:   0.0000\n",
      "step: 64, loss_value: 208.0889, content_loss: 2080.8887, style_loss:   0.0000\n",
      "step: 65, loss_value: 196.6492, content_loss: 1966.4922, style_loss:   0.0000\n",
      "step: 66, loss_value: 185.5733, content_loss: 1855.7328, style_loss:   0.0000\n",
      "step: 67, loss_value: 174.8528, content_loss: 1748.5283, style_loss:   0.0000\n",
      "step: 68, loss_value: 164.5986, content_loss: 1645.9855, style_loss:   0.0000\n",
      "step: 69, loss_value: 154.8431, content_loss: 1548.4314, style_loss:   0.0000\n",
      "step: 70, loss_value: 145.5632, content_loss: 1455.6315, style_loss:   0.0000\n",
      "step: 71, loss_value: 136.7773, content_loss: 1367.7725, style_loss:   0.0000\n",
      "step: 72, loss_value: 128.4921, content_loss: 1284.9207, style_loss:   0.0000\n",
      "step: 73, loss_value: 120.7247, content_loss: 1207.2473, style_loss:   0.0000\n",
      "step: 74, loss_value: 113.4928, content_loss: 1134.9277, style_loss:   0.0000\n",
      "step: 75, loss_value: 106.6927, content_loss: 1066.9275, style_loss:   0.0000\n",
      "step: 76, loss_value: 100.2896, content_loss: 1002.8959, style_loss:   0.0000\n",
      "step: 77, loss_value:  94.2943, content_loss: 942.9431, style_loss:   0.0000\n",
      "step: 78, loss_value:  88.6595, content_loss: 886.5955, style_loss:   0.0000\n",
      "step: 79, loss_value:  83.3099, content_loss: 833.0989, style_loss:   0.0000\n",
      "step: 80, loss_value:  78.3348, content_loss: 783.3475, style_loss:   0.0000\n",
      "step: 81, loss_value:  73.6251, content_loss: 736.2507, style_loss:   0.0000\n",
      "step: 82, loss_value:  69.2061, content_loss: 692.0608, style_loss:   0.0000\n",
      "step: 83, loss_value:  65.0536, content_loss: 650.5358, style_loss:   0.0000\n",
      "step: 84, loss_value:  61.1443, content_loss: 611.4435, style_loss:   0.0000\n",
      "step: 85, loss_value:  57.4846, content_loss: 574.8456, style_loss:   0.0000\n",
      "step: 86, loss_value:  54.0331, content_loss: 540.3309, style_loss:   0.0000\n",
      "step: 87, loss_value:  50.8271, content_loss: 508.2713, style_loss:   0.0000\n",
      "step: 88, loss_value:  47.8368, content_loss: 478.3676, style_loss:   0.0000\n",
      "step: 89, loss_value:  45.0666, content_loss: 450.6660, style_loss:   0.0000\n",
      "step: 90, loss_value:  42.4947, content_loss: 424.9466, style_loss:   0.0000\n",
      "step: 91, loss_value:  40.1092, content_loss: 401.0925, style_loss:   0.0000\n",
      "step: 92, loss_value:  37.8910, content_loss: 378.9102, style_loss:   0.0000\n",
      "step: 93, loss_value:  35.8238, content_loss: 358.2376, style_loss:   0.0000\n",
      "step: 94, loss_value:  33.9053, content_loss: 339.0532, style_loss:   0.0000\n",
      "step: 95, loss_value:  32.1143, content_loss: 321.1431, style_loss:   0.0000\n",
      "step: 96, loss_value:  30.4423, content_loss: 304.4226, style_loss:   0.0000\n",
      "step: 97, loss_value:  28.8857, content_loss: 288.8573, style_loss:   0.0000\n",
      "step: 98, loss_value:  27.4321, content_loss: 274.3209, style_loss:   0.0000\n",
      "step: 99, loss_value:  26.0749, content_loss: 260.7492, style_loss:   0.0000\n",
      "step: 100, loss_value:  24.7990, content_loss: 247.9904, style_loss:   0.0000\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ = \\\n",
    "            sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                         content:content_val,\n",
    "                         style:style_val\n",
    "                     })\n",
    "        # 因为loss_value等，是一个数组，需要通过索引将值去出\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' % \n",
    "                                                                  (step+1,\n",
    "                                                                  loss_value[0],\n",
    "                                                                  content_loss_value[0],\n",
    "                                                                  style_loss_value[0]))\n",
    "        result_img_path = os.path.join(output_dir, 'result_%05d.jpg'%(step+1))\n",
    "        result_val = result.eval(sess)[0] # 将图像取出，因为之前是4维，所以需要使用一个索引0，将其取出\n",
    "\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        # np.clip() numpy.clip(a, a_min, a_max, out=None)[source]\n",
    "        # 其中a是一个数组，后面两个参数分别表示最小和最大值\n",
    "\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        # 保存图像\n",
    "        img.save(result_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前面写的有bug，改不动了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型开始创建……\n",
      "创建模型结束：   0s\n",
      "模型开始创建……\n",
      "创建模型结束：   0s\n",
      "模型开始创建……\n",
      "创建模型结束：   0s\n",
      "step: 1, loss_value: 537377088.0000, content_loss: 264027.0938, style_loss: 1074701.2500\n",
      "step: 2, loss_value: 408541728.0000, content_loss: 280749.5625, style_loss: 817027.3125\n",
      "step: 3, loss_value: 297939040.0000, content_loss: 317524.5312, style_loss: 595814.5625\n",
      "step: 4, loss_value: 206885072.0000, content_loss: 342506.3125, style_loss: 413701.6250\n",
      "step: 5, loss_value: 155116112.0000, content_loss: 356683.9688, style_loss: 310160.9062\n",
      "step: 6, loss_value: 148183344.0000, content_loss: 367696.2500, style_loss: 296293.1562\n",
      "step: 7, loss_value: 156173584.0000, content_loss: 374201.6875, style_loss: 312272.3125\n",
      "step: 8, loss_value: 150005904.0000, content_loss: 374552.6562, style_loss: 299936.9062\n",
      "step: 9, loss_value: 131818376.0000, content_loss: 372169.8750, style_loss: 263562.3125\n",
      "step: 10, loss_value: 114560168.0000, content_loss: 370319.4375, style_loss: 229046.2656\n",
      "step: 11, loss_value: 104513728.0000, content_loss: 370775.3438, style_loss: 208953.2969\n",
      "step: 12, loss_value: 100552464.0000, content_loss: 373929.4375, style_loss: 201030.1406\n",
      "step: 13, loss_value: 98762248.0000, content_loss: 380018.8438, style_loss: 197448.5000\n",
      "step: 14, loss_value: 96188576.0000, content_loss: 387615.2500, style_loss: 192299.6250\n",
      "step: 15, loss_value: 91818880.0000, content_loss: 395040.8438, style_loss: 183558.7500\n",
      "step: 16, loss_value: 85867560.0000, content_loss: 400584.3125, style_loss: 171655.0156\n",
      "step: 17, loss_value: 79153784.0000, content_loss: 404156.3125, style_loss: 158226.7344\n",
      "step: 18, loss_value: 72842152.0000, content_loss: 406562.0938, style_loss: 145602.9844\n",
      "step: 19, loss_value: 67763760.0000, content_loss: 408588.3750, style_loss: 135445.8125\n",
      "step: 20, loss_value: 64000996.0000, content_loss: 410235.0625, style_loss: 127919.9453\n",
      "step: 21, loss_value: 60940852.0000, content_loss: 410641.8750, style_loss: 121799.5781\n",
      "step: 22, loss_value: 57851796.0000, content_loss: 409420.4062, style_loss: 115621.7031\n",
      "step: 23, loss_value: 54410604.0000, content_loss: 407276.2812, style_loss: 108739.7500\n",
      "step: 24, loss_value: 50895372.0000, content_loss: 405418.4062, style_loss: 101709.6641\n",
      "step: 25, loss_value: 47718180.0000, content_loss: 404880.1562, style_loss: 95355.3828\n",
      "step: 26, loss_value: 45120708.0000, content_loss: 406069.1562, style_loss: 90160.2031\n",
      "step: 27, loss_value: 43027776.0000, content_loss: 408688.7812, style_loss: 85973.8125\n",
      "step: 28, loss_value: 41167784.0000, content_loss: 412282.8438, style_loss: 82253.1094\n",
      "step: 29, loss_value: 39257620.0000, content_loss: 415829.5938, style_loss: 78432.0703\n",
      "step: 30, loss_value: 37143960.0000, content_loss: 418608.9688, style_loss: 74204.2031\n",
      "step: 31, loss_value: 34915536.0000, content_loss: 420395.1875, style_loss: 69746.9922\n",
      "step: 32, loss_value: 32781018.0000, content_loss: 421425.4688, style_loss: 65477.7500\n",
      "step: 33, loss_value: 30906466.0000, content_loss: 421826.5625, style_loss: 61728.5664\n",
      "step: 34, loss_value: 29331416.0000, content_loss: 421725.3125, style_loss: 58578.4883\n",
      "step: 35, loss_value: 27962622.0000, content_loss: 421393.5938, style_loss: 55840.9648\n",
      "step: 36, loss_value: 26654446.0000, content_loss: 420908.5000, style_loss: 53224.7109\n",
      "step: 37, loss_value: 25326694.0000, content_loss: 420424.8438, style_loss: 50569.3047\n",
      "step: 38, loss_value: 23981826.0000, content_loss: 420098.4375, style_loss: 47879.6328\n",
      "step: 39, loss_value: 22706364.0000, content_loss: 420124.3125, style_loss: 45328.7031\n",
      "step: 40, loss_value: 21574500.0000, content_loss: 420866.4062, style_loss: 43064.8281\n",
      "step: 41, loss_value: 20580272.0000, content_loss: 422084.3125, style_loss: 41076.1289\n",
      "step: 42, loss_value: 19675050.0000, content_loss: 423338.3750, style_loss: 39265.4336\n",
      "step: 43, loss_value: 18823478.0000, content_loss: 424366.7500, style_loss: 37562.0859\n",
      "step: 44, loss_value: 18012632.0000, content_loss: 425020.0000, style_loss: 35940.2617\n",
      "step: 45, loss_value: 17251274.0000, content_loss: 425318.9375, style_loss: 34417.4844\n",
      "step: 46, loss_value: 16547899.0000, content_loss: 425445.5625, style_loss: 33010.7070\n",
      "step: 47, loss_value: 15908017.0000, content_loss: 425490.4062, style_loss: 31730.9355\n",
      "step: 48, loss_value: 15324062.0000, content_loss: 425540.4062, style_loss: 30563.0156\n",
      "step: 49, loss_value: 14776275.0000, content_loss: 425571.2500, style_loss: 29467.4355\n",
      "step: 50, loss_value: 14257517.0000, content_loss: 425579.4375, style_loss: 28429.9180\n",
      "step: 51, loss_value: 13772831.0000, content_loss: 425625.2188, style_loss: 27460.5352\n",
      "step: 52, loss_value: 13323784.0000, content_loss: 425811.5000, style_loss: 26562.4062\n",
      "step: 53, loss_value: 12914842.0000, content_loss: 426160.2188, style_loss: 25744.4512\n",
      "step: 54, loss_value: 12540777.0000, content_loss: 426596.4062, style_loss: 24996.2344\n",
      "step: 55, loss_value: 12191365.0000, content_loss: 427151.2188, style_loss: 24297.3008\n",
      "step: 56, loss_value: 11858712.0000, content_loss: 427664.8438, style_loss: 23631.8926\n",
      "step: 57, loss_value: 11543518.0000, content_loss: 428037.8438, style_loss: 23001.4277\n",
      "step: 58, loss_value: 11251454.0000, content_loss: 428354.2188, style_loss: 22417.2383\n",
      "step: 59, loss_value: 10981976.0000, content_loss: 428567.3438, style_loss: 21878.2383\n",
      "step: 60, loss_value: 10728891.0000, content_loss: 428685.5625, style_loss: 21372.0449\n",
      "step: 61, loss_value: 10486291.0000, content_loss: 428712.5312, style_loss: 20886.8398\n",
      "step: 62, loss_value: 10253232.0000, content_loss: 428659.6250, style_loss: 20420.7324\n",
      "step: 63, loss_value: 10032046.0000, content_loss: 428638.1250, style_loss: 19978.3633\n",
      "step: 64, loss_value: 9824853.0000, content_loss: 428708.4062, style_loss: 19563.9648\n",
      "step: 65, loss_value: 9631372.0000, content_loss: 428863.0938, style_loss: 19176.9727\n",
      "step: 66, loss_value: 9447802.0000, content_loss: 429045.5625, style_loss: 18809.7949\n",
      "step: 67, loss_value: 9270710.0000, content_loss: 429218.0000, style_loss: 18455.5762\n",
      "step: 68, loss_value: 9099534.0000, content_loss: 429386.1250, style_loss: 18113.1895\n",
      "step: 69, loss_value: 8936377.0000, content_loss: 429570.1562, style_loss: 17786.8398\n",
      "step: 70, loss_value: 8781267.0000, content_loss: 429748.6562, style_loss: 17476.5840\n",
      "step: 71, loss_value: 8632128.0000, content_loss: 429830.7812, style_loss: 17178.2891\n",
      "step: 72, loss_value: 8487061.0000, content_loss: 429810.2500, style_loss: 16888.1602\n",
      "step: 73, loss_value: 8345698.5000, content_loss: 429803.7500, style_loss: 16605.4355\n",
      "step: 74, loss_value: 8208939.0000, content_loss: 429883.5000, style_loss: 16331.9014\n",
      "step: 75, loss_value: 8076967.5000, content_loss: 430050.0312, style_loss: 16067.9248\n",
      "step: 76, loss_value: 7950517.5000, content_loss: 430292.7500, style_loss: 15814.9756\n",
      "step: 77, loss_value: 7829669.0000, content_loss: 430545.3125, style_loss: 15573.2285\n",
      "step: 78, loss_value: 7713150.0000, content_loss: 430775.4375, style_loss: 15340.1445\n",
      "step: 79, loss_value: 7600429.0000, content_loss: 431014.8438, style_loss: 15114.6553\n",
      "step: 80, loss_value: 7491370.5000, content_loss: 431279.2500, style_loss: 14896.4854\n",
      "step: 81, loss_value: 7386226.0000, content_loss: 431544.2500, style_loss: 14686.1426\n",
      "step: 82, loss_value: 7284553.0000, content_loss: 431763.6875, style_loss: 14482.7529\n",
      "step: 83, loss_value: 7185826.0000, content_loss: 431909.9062, style_loss: 14285.2695\n",
      "step: 84, loss_value: 7089580.5000, content_loss: 432029.0312, style_loss: 14092.7549\n",
      "step: 85, loss_value: 6995831.5000, content_loss: 432171.0312, style_loss: 13905.2295\n",
      "step: 86, loss_value: 6905174.5000, content_loss: 432355.2188, style_loss: 13723.8779\n",
      "step: 87, loss_value: 6817537.0000, content_loss: 432535.7812, style_loss: 13548.5674\n",
      "step: 88, loss_value: 6732764.5000, content_loss: 432703.0312, style_loss: 13378.9883\n",
      "step: 89, loss_value: 6650160.0000, content_loss: 432840.0000, style_loss: 13213.7520\n",
      "step: 90, loss_value: 6569636.5000, content_loss: 432944.6562, style_loss: 13052.6836\n",
      "step: 91, loss_value: 6491212.0000, content_loss: 433033.9062, style_loss: 12895.8174\n",
      "step: 92, loss_value: 6414836.5000, content_loss: 433116.7500, style_loss: 12743.0498\n",
      "step: 93, loss_value: 6340684.5000, content_loss: 433168.2812, style_loss: 12594.7354\n",
      "step: 94, loss_value: 6268902.5000, content_loss: 433246.2188, style_loss: 12451.1562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 95, loss_value: 6198822.5000, content_loss: 433354.1250, style_loss: 12310.9736\n",
      "step: 96, loss_value: 6130402.5000, content_loss: 433476.9062, style_loss: 12174.1104\n",
      "step: 97, loss_value: 6063570.5000, content_loss: 433593.3750, style_loss: 12040.4219\n",
      "step: 98, loss_value: 5998035.5000, content_loss: 433684.4062, style_loss: 11909.3340\n",
      "step: 99, loss_value: 5933337.5000, content_loss: 433760.2500, style_loss: 11779.9229\n",
      "step: 100, loss_value: 5869703.0000, content_loss: 433852.5000, style_loss: 11652.6357\n",
      "step: 101, loss_value: 5807230.0000, content_loss: 433954.6562, style_loss: 11527.6689\n",
      "step: 102, loss_value: 5746001.0000, content_loss: 434048.6562, style_loss: 11405.1924\n",
      "step: 103, loss_value: 5685688.0000, content_loss: 434142.7812, style_loss: 11284.5469\n",
      "step: 104, loss_value: 5626399.0000, content_loss: 434228.6875, style_loss: 11165.9521\n",
      "step: 105, loss_value: 5568007.0000, content_loss: 434311.5000, style_loss: 11049.1523\n",
      "step: 106, loss_value: 5510638.5000, content_loss: 434386.8438, style_loss: 10934.4004\n",
      "step: 107, loss_value: 5454475.0000, content_loss: 434456.2188, style_loss: 10822.0586\n",
      "step: 108, loss_value: 5399312.5000, content_loss: 434528.2500, style_loss: 10711.7188\n",
      "step: 109, loss_value: 5345078.0000, content_loss: 434619.2188, style_loss: 10603.2324\n",
      "step: 110, loss_value: 5291858.0000, content_loss: 434722.0000, style_loss: 10496.7725\n",
      "step: 111, loss_value: 5239635.0000, content_loss: 434813.5625, style_loss: 10392.3066\n",
      "step: 112, loss_value: 5188253.0000, content_loss: 434877.8750, style_loss: 10289.5303\n",
      "step: 113, loss_value: 5137625.0000, content_loss: 434911.8438, style_loss: 10188.2676\n",
      "step: 114, loss_value: 5087833.5000, content_loss: 434947.9062, style_loss: 10088.6768\n",
      "step: 115, loss_value: 5038892.0000, content_loss: 435009.7500, style_loss: 9990.7822\n",
      "step: 116, loss_value: 4990721.5000, content_loss: 435092.9062, style_loss: 9894.4238\n",
      "step: 117, loss_value: 4943125.0000, content_loss: 435166.4375, style_loss: 9799.2168\n",
      "step: 118, loss_value: 4896152.0000, content_loss: 435218.3750, style_loss: 9705.2598\n",
      "step: 119, loss_value: 4849566.0000, content_loss: 435249.4375, style_loss: 9612.0820\n",
      "step: 120, loss_value: 4803435.0000, content_loss: 435291.2188, style_loss: 9519.8125\n",
      "step: 121, loss_value: 4757876.0000, content_loss: 435328.8438, style_loss: 9428.6855\n",
      "step: 122, loss_value: 4713001.5000, content_loss: 435357.7500, style_loss: 9338.9307\n",
      "step: 123, loss_value: 4668735.0000, content_loss: 435380.6875, style_loss: 9250.3936\n",
      "step: 124, loss_value: 4625025.0000, content_loss: 435424.7812, style_loss: 9162.9648\n",
      "step: 125, loss_value: 4582001.0000, content_loss: 435481.7812, style_loss: 9076.9062\n",
      "step: 126, loss_value: 4539426.0000, content_loss: 435541.1875, style_loss: 8991.7441\n",
      "step: 127, loss_value: 4497276.5000, content_loss: 435604.4062, style_loss: 8907.4316\n",
      "step: 128, loss_value: 4455622.0000, content_loss: 435674.0938, style_loss: 8824.1094\n",
      "step: 129, loss_value: 4414351.5000, content_loss: 435737.9688, style_loss: 8741.5547\n",
      "step: 130, loss_value: 4373548.5000, content_loss: 435793.5625, style_loss: 8659.9385\n",
      "step: 131, loss_value: 4333107.5000, content_loss: 435842.2188, style_loss: 8579.0469\n",
      "step: 132, loss_value: 4293070.0000, content_loss: 435886.0000, style_loss: 8498.9629\n",
      "step: 133, loss_value: 4253561.5000, content_loss: 435943.8438, style_loss: 8419.9336\n",
      "step: 134, loss_value: 4214697.5000, content_loss: 436034.2812, style_loss: 8342.1885\n",
      "step: 135, loss_value: 4176224.2500, content_loss: 436119.0938, style_loss: 8265.2246\n",
      "step: 136, loss_value: 4138258.5000, content_loss: 436183.5625, style_loss: 8189.2803\n",
      "step: 137, loss_value: 4100828.0000, content_loss: 436246.5625, style_loss: 8114.4067\n",
      "step: 138, loss_value: 4063959.7500, content_loss: 436320.0000, style_loss: 8040.6553\n",
      "step: 139, loss_value: 4027611.0000, content_loss: 436387.8438, style_loss: 7967.9443\n",
      "step: 140, loss_value: 3991730.2500, content_loss: 436443.9062, style_loss: 7896.1714\n",
      "step: 141, loss_value: 3956257.7500, content_loss: 436508.5625, style_loss: 7825.2139\n",
      "step: 142, loss_value: 3921232.7500, content_loss: 436584.2812, style_loss: 7755.1484\n",
      "step: 143, loss_value: 3886615.5000, content_loss: 436651.9062, style_loss: 7685.9004\n",
      "step: 144, loss_value: 3852348.0000, content_loss: 436716.5625, style_loss: 7617.3525\n",
      "step: 145, loss_value: 3818400.2500, content_loss: 436776.4062, style_loss: 7549.4448\n",
      "step: 146, loss_value: 3784916.0000, content_loss: 436837.6250, style_loss: 7482.4644\n",
      "step: 147, loss_value: 3751770.0000, content_loss: 436898.6875, style_loss: 7416.1606\n",
      "step: 148, loss_value: 3719038.0000, content_loss: 436958.5000, style_loss: 7350.6846\n",
      "step: 149, loss_value: 3686578.0000, content_loss: 436998.3750, style_loss: 7285.7563\n",
      "step: 150, loss_value: 3654520.2500, content_loss: 437026.2188, style_loss: 7221.6353\n",
      "step: 151, loss_value: 3622905.0000, content_loss: 437052.4062, style_loss: 7158.3994\n",
      "step: 152, loss_value: 3591558.0000, content_loss: 437089.7188, style_loss: 7095.6982\n",
      "step: 153, loss_value: 3560535.7500, content_loss: 437141.3125, style_loss: 7033.6431\n",
      "step: 154, loss_value: 3529801.2500, content_loss: 437185.3438, style_loss: 6972.1655\n",
      "step: 155, loss_value: 3499467.0000, content_loss: 437228.7500, style_loss: 6911.4883\n",
      "step: 156, loss_value: 3469517.7500, content_loss: 437265.8438, style_loss: 6851.5825\n",
      "step: 157, loss_value: 3439883.2500, content_loss: 437292.0312, style_loss: 6792.3081\n",
      "step: 158, loss_value: 3410543.7500, content_loss: 437323.4375, style_loss: 6733.6230\n",
      "step: 159, loss_value: 3381605.2500, content_loss: 437348.5000, style_loss: 6675.7412\n",
      "step: 160, loss_value: 3352941.2500, content_loss: 437384.5000, style_loss: 6618.4053\n",
      "step: 161, loss_value: 3324495.5000, content_loss: 437423.8750, style_loss: 6561.5059\n",
      "step: 162, loss_value: 3296261.0000, content_loss: 437471.4375, style_loss: 6505.0273\n",
      "step: 163, loss_value: 3268405.5000, content_loss: 437518.1562, style_loss: 6449.3076\n",
      "step: 164, loss_value: 3240903.2500, content_loss: 437569.1562, style_loss: 6394.2925\n",
      "step: 165, loss_value: 3213721.0000, content_loss: 437609.5000, style_loss: 6339.9199\n",
      "step: 166, loss_value: 3186763.5000, content_loss: 437642.6250, style_loss: 6285.9985\n",
      "step: 167, loss_value: 3159974.0000, content_loss: 437681.6250, style_loss: 6232.4116\n",
      "step: 168, loss_value: 3133355.5000, content_loss: 437734.6250, style_loss: 6179.1641\n",
      "step: 169, loss_value: 3106965.5000, content_loss: 437775.3438, style_loss: 6126.3760\n",
      "step: 170, loss_value: 3080725.5000, content_loss: 437797.2188, style_loss: 6073.8916\n",
      "step: 171, loss_value: 3054724.0000, content_loss: 437830.6562, style_loss: 6021.8818\n",
      "step: 172, loss_value: 3028928.7500, content_loss: 437878.6875, style_loss: 5970.2822\n",
      "step: 173, loss_value: 3003314.5000, content_loss: 437928.9375, style_loss: 5919.0430\n",
      "step: 174, loss_value: 2977886.7500, content_loss: 437982.6250, style_loss: 5868.1772\n",
      "step: 175, loss_value: 2952583.7500, content_loss: 438027.0312, style_loss: 5817.5620\n",
      "step: 176, loss_value: 2927423.2500, content_loss: 438066.6875, style_loss: 5767.2329\n",
      "step: 177, loss_value: 2902405.2500, content_loss: 438116.8125, style_loss: 5717.1870\n",
      "step: 178, loss_value: 2877621.0000, content_loss: 438173.3438, style_loss: 5667.6074\n",
      "step: 179, loss_value: 2853096.2500, content_loss: 438228.6250, style_loss: 5618.5469\n",
      "step: 180, loss_value: 2828859.5000, content_loss: 438292.4375, style_loss: 5570.0605\n",
      "step: 181, loss_value: 2804930.0000, content_loss: 438353.5625, style_loss: 5522.1895\n",
      "step: 182, loss_value: 2781183.2500, content_loss: 438396.1562, style_loss: 5474.6875\n",
      "step: 183, loss_value: 2757606.0000, content_loss: 438428.4375, style_loss: 5427.5264\n",
      "step: 184, loss_value: 2734187.5000, content_loss: 438485.1562, style_loss: 5380.6782\n",
      "step: 185, loss_value: 2710996.2500, content_loss: 438560.0938, style_loss: 5334.2803\n",
      "step: 186, loss_value: 2688111.5000, content_loss: 438623.4375, style_loss: 5288.4985\n",
      "step: 187, loss_value: 2665388.5000, content_loss: 438668.8125, style_loss: 5243.0430\n",
      "step: 188, loss_value: 2642907.2500, content_loss: 438723.2500, style_loss: 5198.0698\n",
      "step: 189, loss_value: 2620622.0000, content_loss: 438792.5312, style_loss: 5153.4854\n",
      "step: 190, loss_value: 2598519.7500, content_loss: 438836.5312, style_loss: 5109.2720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 191, loss_value: 2576629.5000, content_loss: 438873.9688, style_loss: 5065.4839\n",
      "step: 192, loss_value: 2554920.5000, content_loss: 438933.6250, style_loss: 5022.0547\n",
      "step: 193, loss_value: 2533440.5000, content_loss: 438992.6562, style_loss: 4979.0825\n",
      "step: 194, loss_value: 2512158.5000, content_loss: 439025.6250, style_loss: 4936.5122\n",
      "step: 195, loss_value: 2491068.7500, content_loss: 439049.0312, style_loss: 4894.3276\n",
      "step: 196, loss_value: 2470239.2500, content_loss: 439082.2500, style_loss: 4852.6621\n",
      "step: 197, loss_value: 2449632.0000, content_loss: 439112.2500, style_loss: 4811.4414\n",
      "step: 198, loss_value: 2429155.0000, content_loss: 439137.7812, style_loss: 4770.4824\n",
      "step: 199, loss_value: 2408912.2500, content_loss: 439180.6562, style_loss: 4729.9883\n",
      "step: 200, loss_value: 2388854.5000, content_loss: 439229.8750, style_loss: 4689.8628\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "# VGG 自带的一个常量，之前VGG训练通过归一化，所以现在同样需要作此操作\n",
    "VGG_MEAN = [103.939, 116.779, 123.68] # rgb 三通道的均值\n",
    "\n",
    "class VGGNet():\n",
    "    '''\n",
    "    创建 vgg16 网络 结构\n",
    "    从模型中载入参数\n",
    "    '''\n",
    "    def __init__(self, data_dict):\n",
    "        '''\n",
    "        传入vgg16模型\n",
    "        :param data_dict: vgg16.npy (字典类型)\n",
    "        '''\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        '''\n",
    "        得到对应名称的卷积层\n",
    "        :param name: 卷积层名称\n",
    "        :return: 该卷积层输出\n",
    "        '''\n",
    "        return tf.constant(self.data_dict[name][0], name = 'conv')\n",
    "\n",
    "    def get_fc_weight(self, name):\n",
    "        '''\n",
    "        获得名字为name的全连接层权重\n",
    "        :param name: 连接层名称\n",
    "        :return: 该层权重\n",
    "        '''\n",
    "        return tf.constant(self.data_dict[name][0], name = 'fc')\n",
    "\n",
    "    def get_bias(self, name):\n",
    "        '''\n",
    "        获得名字为name的全连接层偏置\n",
    "        :param name: 连接层名称\n",
    "        :return: 该层偏置\n",
    "        '''\n",
    "        return tf.constant(self.data_dict[name][1], name = 'bias')\n",
    "\n",
    "\n",
    "    def conv_layer(self, x, name):\n",
    "        '''\n",
    "        创建一个卷积层\n",
    "        :param x:\n",
    "        :param name:\n",
    "        :return:\n",
    "        '''\n",
    "        # 在写计算图模型的时候，加一些必要的 name_scope，这是一个比较好的编程规范\n",
    "        # 可以防止命名冲突， 二可视化计算图的时候比较清楚\n",
    "        with tf.name_scope(name):\n",
    "            # 获得 w 和 b\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "\n",
    "            # 进行卷积计算\n",
    "            h = tf.nn.conv2d(x, conv_w, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            '''\n",
    "            因为此刻的 w 和 b 是从外部传递进来，所以使用 tf.nn.conv2d()\n",
    "            tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu = None, name = None) 参数说明：\n",
    "            input 输入的tensor， 格式[batch, height, width, channel]\n",
    "            filter 卷积核 [filter_height, filter_width, in_channels, out_channels] \n",
    "                分别是：卷积核高，卷积核宽，输入通道数，输出通道数\n",
    "            strides 步长 卷积时在图像每一维度的步长，长度为4\n",
    "            padding 参数可选择 “SAME” “VALID”\n",
    "            \n",
    "            '''\n",
    "            # 加上偏置\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            # 使用激活函数\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "\n",
    "\n",
    "    def pooling_layer(self, x, name):\n",
    "        '''\n",
    "        创建池化层\n",
    "        :param x: 输入的tensor\n",
    "        :param name: 池化层名称\n",
    "        :return: tensor\n",
    "        '''\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize = [1, 2, 2, 1], # 核参数， 注意：都是4维\n",
    "                              strides = [1, 2, 2, 1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name\n",
    "                              )\n",
    "\n",
    "    def fc_layer(self, x, name, activation = tf.nn.relu):\n",
    "        '''\n",
    "        创建全连接层\n",
    "        :param x: 输入tensor\n",
    "        :param name: 全连接层名称\n",
    "        :param activation: 激活函数名称\n",
    "        :return: 输出tensor\n",
    "        '''\n",
    "        with tf.name_scope(name, activation):\n",
    "            # 获取全连接层的 w 和 b\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            # 矩阵相乘 计算\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            #　添加偏置\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            # 因为最后一层是没有激活函数ｒｅｌｕ的，所以在此要做出判断\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "\n",
    "    def flatten_layer(self, x, name):\n",
    "        '''\n",
    "        展平\n",
    "        :param x: input_tensor\n",
    "        :param name:\n",
    "        :return: 二维矩阵\n",
    "        '''\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            # 计算后三维合并后的大小\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            # 形成一个二维矩阵\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x\n",
    "\n",
    "    def build(self, x_rgb):\n",
    "        '''\n",
    "        创建vgg16 网络\n",
    "        :param x_rgb: [1, 224, 224, 3]\n",
    "        :return:\n",
    "        '''\n",
    "        start_time = time.time()\n",
    "        print('模型开始创建……')\n",
    "        # 将输入图像进行处理，将每个通道减去均值\n",
    "        r, g, b = tf.split(x_rgb, [1, 1, 1], axis = 3)\n",
    "        '''\n",
    "        tf.split(value, num_or_size_split, axis=0)用法：\n",
    "        value:输入的Tensor\n",
    "        num_or_size_split:有两种用法：\n",
    "            1.直接传入一个整数，代表会被切成几个张量，切割的维度有axis指定\n",
    "            2.传入一个向量，向量长度就是被切的份数。传入向量的好处在于，可以指定每一份有多少元素\n",
    "        axis, 指定从哪一个维度切割\n",
    "        因此，上一句的意思就是从第4维切分，分为3份，每一份只有1个元素\n",
    "        '''\n",
    "        # 将 处理后的通道再次合并起来\n",
    "        x_bgr = tf.concat([b - VGG_MEAN[0], g - VGG_MEAN[1], r - VGG_MEAN[2]], axis = 3)\n",
    "\n",
    "#        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "\n",
    "        # 开始构建卷积层\n",
    "        # vgg16 的网络结构\n",
    "        # 第一层：2个卷积层 1个pooling层\n",
    "        # 第二层：2个卷积层 1个pooling层\n",
    "        # 第三层：3个卷积层 1个pooling层\n",
    "        # 第四层：3个卷积层 1个pooling层\n",
    "        # 第五层：3个卷积层 1个pooling层\n",
    "        # 第六层： 全连接\n",
    "        # 第七层： 全连接\n",
    "        # 第八层： 全连接\n",
    "\n",
    "        # 这些变量名称不能乱取，必须要和vgg16模型保持一致\n",
    "        # 另外，将这些卷积层用self.的形式，方便以后取用方便\n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "\n",
    "        ''' 因为风格转换只需要 卷积层  的数据\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation = None)\n",
    "        self.prob = tf.nn.softmax(self.fc8, name = 'prob')\n",
    "        '''\n",
    "\n",
    "\n",
    "        print('创建模型结束：%4ds' % (time.time() - start_time))\n",
    "\n",
    "# 指定 model 路径\n",
    "vgg16_npy_pyth = './vgg16.npy'\n",
    "# 内容图像 路径\n",
    "content_img_path = './source_images/style_wave.jpg'\n",
    "# 风格图像路径\n",
    "style_img_path = './source_images/source.jpg'\n",
    "\n",
    "# 训练的步数\n",
    "num_steps = 200\n",
    "# 指定学习率\n",
    "learning_rate = 10\n",
    "\n",
    "# 设置 两个 参数\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "# 输入 目录\n",
    "output_dir = './run_style_transfer'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "def initial_result(shape, mean, stddev):\n",
    "    '''\n",
    "    定义一个初始化好的随机图片，然后在该图片上不停的梯度下降来得到效果。\n",
    "    :param shape: 输入形状\n",
    "    :param mean: 均值\n",
    "    :param stddev: 方法\n",
    "    :return: 图片\n",
    "    '''\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev) # 一个截断的正态分布\n",
    "    '''\n",
    "    tf.truncated_normal(shape, mean, stddev) 生成截断的生态分布函数\n",
    "    如果产生的正态分布值和均值差值大于二倍的标准差，那就重新生成。\n",
    "    '''\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    '''\n",
    "    读取图片\n",
    "    :param img_name: 图片路径\n",
    "    :return: 4维矩阵\n",
    "    '''\n",
    "    img = Image.open(img_name)\n",
    "    # 图像为三通道（224， 244， 3），但是需要转化为4维\n",
    "    img = img.resize((224, 224))\n",
    "    np_img = np.array(img) # 224, 224, 3\n",
    "    np_img = np.asarray([np_img], dtype = np.int32) # 这个函数作用不太理解 (1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    '''\n",
    "    计算 gram 矩阵\n",
    "    :param x: 特征图，shape：[1, width, height, channel]\n",
    "    :return:\n",
    "    '''\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    # 这里求出来的是 每一个feature map之间的相似度\n",
    "    features = tf.reshape(x, [b, h * w, ch]) # 将二三维的维度合并，已组成三维\n",
    "    # 相似度矩阵 方法： 将矩阵转置为[ch, b*w], 再乘原矩阵，最后的矩阵是[ch , ch]\n",
    "    # 防止矩阵数值过大，除以一个常数\n",
    "    gram = tf.matmul(features, features, adjoint_a = True) / tf.constant(ch * w * h, tf.float32) # 参数3， 表示将第一个参数转置\n",
    "    return gram\n",
    "\n",
    "\n",
    "# 生成一个图像，均值为127.5，方差为20\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "\n",
    "# 读取 内容图像 和 风格图像\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape = [1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape = [1, 224, 224, 3])\n",
    "\n",
    "# 载入模型， 注意：在python3中，需要添加一句： encoding='latin1'\n",
    "data_dict = np.load(vgg16_npy_pyth, encoding='latin1',allow_pickle=True).item()\n",
    "\n",
    "\n",
    "# 创建这三张图像的 vgg 对象\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "# 创建 每个 神经网络\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "# 提取哪些层特征\n",
    "# 需要注意的是：内容特征抽取的层数和结果特征抽取的层数必须相同\n",
    "# 风格特征抽取的层数和结果特征抽取的层数必须相同\n",
    "content_features = [\n",
    "#                     vgg_for_content.conv1_2,\n",
    "#                     vgg_for_content.conv2_2,\n",
    "                    vgg_for_content.conv3_3,\n",
    "#                     vgg_for_content.conv4_3,\n",
    "#                     vgg_for_content.conv5_3,\n",
    "                    ]\n",
    "\n",
    "result_content_features = [\n",
    "#                           vgg_for_result.conv1_2,\n",
    "#                           vgg_for_result.conv2_2,\n",
    "                          vgg_for_result.conv3_3,\n",
    "#                           vgg_for_result.conv4_3,\n",
    "#                           vgg_for_result.conv5_3\n",
    "                          ]\n",
    "\n",
    "# feature_size, [1, width, height, channel]\n",
    "style_features = [\n",
    "                          vgg_for_style.conv1_2,\n",
    "                          vgg_for_style.conv2_2,\n",
    "                          vgg_for_style.conv3_3,\n",
    "                          vgg_for_style.conv4_3,\n",
    "                          # vgg_for_style.conv5_3\n",
    "                          ]\n",
    "\n",
    "# 为列表中每一个元素，都计算 gram\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "                          vgg_for_result.conv1_2,\n",
    "                          vgg_for_result.conv2_2,\n",
    "                          vgg_for_result.conv3_3,\n",
    "                          vgg_for_result.conv4_3,\n",
    "                          # vgg_for_result.conv5_3\n",
    "                          ]\n",
    "\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# 计算内容损失\n",
    "# 卷积层的形状 shape:[1, width, height, channel], 需要在三个通道上做平均\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_)**2, axis = [1, 2, 3])\n",
    "\n",
    "# 风格内容损失\n",
    "\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    # 因为在计算gram矩阵的时候，降低了一维，所以，只需要在[1, 2]两个维度求均值即可\n",
    "    style_loss += tf.reduce_mean( (s - s_)** 2, [1, 2] )\n",
    "\n",
    "\n",
    "# 总的损失函数\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "\n",
    "\n",
    "train_op = tf.train.AdamOptimizer( learning_rate ).minimize(loss)\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ = \\\n",
    "            sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                         content:content_val,\n",
    "                         style:style_val\n",
    "                     })\n",
    "        # 因为loss_value等，是一个数组，需要通过索引将值去出\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' % (step+1,\n",
    "                                                                  loss_value[0],\n",
    "                                                                  content_loss_value[0],\n",
    "                                                                  style_loss_value[0]))\n",
    "        result_img_path = os.path.join(output_dir, 'result_%05d.jpg'%(step+1))\n",
    "        result_val = result.eval(sess)[0] # 将图像取出，因为之前是4维，所以需要使用一个索引0，将其取出\n",
    "\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        # np.clip() numpy.clip(a, a_min, a_max, out=None)[source]\n",
    "        # 其中a是一个数组，后面两个参数分别表示最小和最大值\n",
    "\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        # 保存图像\n",
    "        img.save(result_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
