{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "       load parameters from pre-train models.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "    \n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    \n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    \n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding='SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "    \n",
    "    \n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize = [1,2,2,1],\n",
    "                              strides = [1,2,2,1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name)\n",
    "    \n",
    "    def fc_layer(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "    \n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x\n",
    "    \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"Build VGG16 network structure.\n",
    "        Parameters:\n",
    "        - x_rgb: [1, 224, 224, 3]\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('building model ...')\n",
    "        \n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis=3)\n",
    "        x_bgr = tf.concat(\n",
    "            [b - VGG_MEAN[0],\n",
    "             g - VGG_MEAN[1],\n",
    "             r - VGG_MEAN[2]],\n",
    "            axis = 3)\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation=None)\n",
    "        self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "  \n",
    "        \n",
    "        print ('building model finished: %4ds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16_npy_path = './vgg16.npy'\n",
    "# data_dict = np.load(vgg16_npy_path,allow_pickle=True, encoding='latin1').item()\n",
    "\n",
    "# vgg16_for_result = VGGNet(data_dict)\n",
    "# content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "# vgg16_for_result.build(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_npy_path = './vgg16.npy'\n",
    "content_img_path = './source_images/2.jpg'\n",
    "style_img_path = './source_images/1.jpg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = './run_style_transfer'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model ...\n",
      "building model finished:    4s\n",
      "building model ...\n",
      "building model finished:    4s\n",
      "building model ...\n",
      "building model finished:    4s\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "def initial_result(shape, mean, stddev):\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img) # (224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) # (1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calculates gram matrix\n",
    "    Args: \n",
    "    - x: features extractedd from VGG Net. shape[1, width, height, ch]\n",
    "    \"\"\"\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, h*w, ch]) # [ch, ch] -> (i, j)\n",
    "    # [h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch]\n",
    "    gram = tf.matmul(features, features, adjoint_a=True) / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "    \n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "# content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "# style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "data_dict = np.load(vgg16_npy_path,allow_pickle=True, encoding='latin1').item()\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "content_features = [\n",
    "            vgg_for_content.conv1_2,\n",
    "            vgg_for_content.conv2_2,\n",
    "#             vgg_for_content.conv3_3,\n",
    "#             vgg_for_content.conv4_3,\n",
    "#             vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "\n",
    "result_content_features = [\n",
    "            vgg_for_result.conv1_2,\n",
    "            vgg_for_result.conv2_2,\n",
    "#             vgg_for_result.conv3_3,\n",
    "#             vgg_for_result.conv4_3,\n",
    "#             vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# feature_size, [1, width, height, channel]\n",
    "style_features = [ \n",
    "#             vgg_for_style.conv1_2,\n",
    "#             vgg_for_style.conv2_2,\n",
    "#             vgg_for_style.conv3_3,\n",
    "            vgg_for_style.conv4_3,\n",
    "#             vgg_for_style.conv5_3\n",
    "]\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "#             vgg_for_style.conv1_2,\n",
    "#             vgg_for_style.conv2_2,\n",
    "#             vgg_for_style.conv3_3,\n",
    "            vgg_for_style.conv4_3,\n",
    "#             vgg_for_style.conv5_3\n",
    "]\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# zip: [1,2], [3,4], zip([1,2], [3,4]) -> [(1,3), (2,4)]\n",
    "# shape: [1, width, height, channel]\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])\n",
    "    \n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "    \n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ = \\\n",
    "            sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                         content:content_val,\n",
    "                         style:style_val\n",
    "                     })\n",
    "        # 因为loss_value等，是一个数组，需要通过索引将值去出\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' % (step+1,\n",
    "                                                                  loss_value[0],\n",
    "                                                                  content_loss_value[0],\n",
    "                                                                  style_loss_value[0]))\n",
    "        result_img_path = os.path.join(output_dir, 'result_%05d.jpg'%(step+1))\n",
    "        result_val = result.eval(sess)[0] # 将图像取出，因为之前是4维，所以需要使用一个索引0，将其取出\n",
    "\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        # np.clip() numpy.clip(a, a_min, a_max, out=None)[source]\n",
    "        # 其中a是一个数组，后面两个参数分别表示最小和最大值\n",
    "\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        # 保存图像\n",
    "        img.save(result_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型开始创建……\n",
      "创建模型结束：   0s\n",
      "模型开始创建……\n",
      "创建模型结束：   0s\n",
      "模型开始创建……\n",
      "创建模型结束：   0s\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "step: 1, loss_value: 37472.2422, content_loss: 312279.3125, style_loss:  12.4886\n",
      "step: 2, loss_value: 30786.5332, content_loss: 247810.4375, style_loss:  12.0110\n",
      "step: 3, loss_value: 25431.5469, content_loss: 203288.5000, style_loss:  10.2054\n",
      "step: 4, loss_value: 21239.9043, content_loss: 175485.1250, style_loss:   7.3828\n",
      "step: 5, loss_value: 18508.1289, content_loss: 156942.1094, style_loss:   5.6278\n",
      "step: 6, loss_value: 16594.0020, content_loss: 143771.3906, style_loss:   4.4337\n",
      "step: 7, loss_value: 15294.2793, content_loss: 132883.9219, style_loss:   4.0118\n",
      "step: 8, loss_value: 14043.8408, content_loss: 123464.8750, style_loss:   3.3947\n",
      "step: 9, loss_value: 13155.0488, content_loss: 114453.3516, style_loss:   3.4194\n",
      "step: 10, loss_value: 12138.3203, content_loss: 106128.2969, style_loss:   3.0510\n",
      "step: 11, loss_value: 11358.4873, content_loss: 97938.7734, style_loss:   3.1292\n",
      "step: 12, loss_value: 10527.7539, content_loss: 90738.0625, style_loss:   2.9079\n",
      "step: 13, loss_value: 9901.6660, content_loss: 84116.2969, style_loss:   2.9801\n",
      "step: 14, loss_value: 9307.0967, content_loss: 78330.9531, style_loss:   2.9480\n",
      "step: 15, loss_value: 8887.5098, content_loss: 73152.1562, style_loss:   3.1446\n",
      "step: 16, loss_value: 8424.7441, content_loss: 68816.4531, style_loss:   3.0862\n",
      "step: 17, loss_value: 8011.3594, content_loss: 65157.6523, style_loss:   2.9912\n",
      "step: 18, loss_value: 7856.6274, content_loss: 61522.7617, style_loss:   3.4087\n",
      "step: 19, loss_value: 7324.9077, content_loss: 58615.4531, style_loss:   2.9267\n",
      "step: 20, loss_value: 7042.7168, content_loss: 55918.3008, style_loss:   2.9018\n",
      "step: 21, loss_value: 6744.4912, content_loss: 53444.5664, style_loss:   2.8001\n",
      "step: 22, loss_value: 6592.6328, content_loss: 50781.4609, style_loss:   3.0290\n",
      "step: 23, loss_value: 6489.0581, content_loss: 48616.8555, style_loss:   3.2547\n",
      "step: 24, loss_value: 6189.8774, content_loss: 47229.5078, style_loss:   2.9339\n",
      "step: 25, loss_value: 6026.6572, content_loss: 46030.0703, style_loss:   2.8473\n",
      "step: 26, loss_value: 5833.9463, content_loss: 44635.1367, style_loss:   2.7409\n",
      "step: 27, loss_value: 5646.7217, content_loss: 43125.8594, style_loss:   2.6683\n",
      "step: 28, loss_value: 5587.2227, content_loss: 41214.0195, style_loss:   2.9316\n",
      "step: 29, loss_value: 5435.0361, content_loss: 39991.4648, style_loss:   2.8718\n",
      "step: 30, loss_value: 5576.5234, content_loss: 38488.1758, style_loss:   3.4554\n",
      "step: 31, loss_value: 5171.5347, content_loss: 38009.1055, style_loss:   2.7412\n",
      "step: 32, loss_value: 5098.9722, content_loss: 37627.3438, style_loss:   2.6725\n",
      "step: 33, loss_value: 5119.7603, content_loss: 36681.9883, style_loss:   2.9031\n",
      "step: 34, loss_value: 4976.8599, content_loss: 35948.7344, style_loss:   2.7640\n",
      "step: 35, loss_value: 4996.1177, content_loss: 34875.8281, style_loss:   3.0171\n",
      "step: 36, loss_value: 4762.2222, content_loss: 34376.9375, style_loss:   2.6491\n",
      "step: 37, loss_value: 4719.6572, content_loss: 33580.8867, style_loss:   2.7231\n",
      "step: 38, loss_value: 4741.8979, content_loss: 32783.8516, style_loss:   2.9270\n",
      "step: 39, loss_value: 4694.6226, content_loss: 32196.5273, style_loss:   2.9499\n",
      "step: 40, loss_value: 4754.5303, content_loss: 31781.2266, style_loss:   3.1528\n",
      "step: 41, loss_value: 4532.1411, content_loss: 31723.3184, style_loss:   2.7196\n",
      "step: 42, loss_value: 4510.8135, content_loss: 31558.5039, style_loss:   2.7099\n",
      "step: 43, loss_value: 4507.2573, content_loss: 30948.4883, style_loss:   2.8248\n",
      "step: 44, loss_value: 4330.2446, content_loss: 30299.8438, style_loss:   2.6005\n",
      "step: 45, loss_value: 4293.9434, content_loss: 29336.1191, style_loss:   2.7207\n",
      "step: 46, loss_value: 4184.3477, content_loss: 28712.9453, style_loss:   2.6261\n",
      "step: 47, loss_value: 4233.5698, content_loss: 27987.4492, style_loss:   2.8697\n",
      "step: 48, loss_value: 4201.2910, content_loss: 27747.8867, style_loss:   2.8530\n",
      "step: 49, loss_value: 4211.1260, content_loss: 27605.9961, style_loss:   2.9011\n",
      "step: 50, loss_value: 4156.8525, content_loss: 27563.3008, style_loss:   2.8010\n",
      "step: 51, loss_value: 4097.3945, content_loss: 27578.2129, style_loss:   2.6791\n",
      "step: 52, loss_value: 4161.7412, content_loss: 27112.5918, style_loss:   2.9010\n",
      "step: 53, loss_value: 4063.7905, content_loss: 26972.0352, style_loss:   2.7332\n",
      "step: 54, loss_value: 4084.9309, content_loss: 26499.2578, style_loss:   2.8700\n",
      "step: 55, loss_value: 3956.7793, content_loss: 26444.7891, style_loss:   2.6246\n",
      "step: 56, loss_value: 3944.7939, content_loss: 26265.9297, style_loss:   2.6364\n",
      "step: 57, loss_value: 3964.6289, content_loss: 25769.4316, style_loss:   2.7754\n",
      "step: 58, loss_value: 3914.1133, content_loss: 25465.0938, style_loss:   2.7352\n",
      "step: 59, loss_value: 3947.0972, content_loss: 25185.5840, style_loss:   2.8571\n",
      "step: 60, loss_value: 3846.3438, content_loss: 25219.6152, style_loss:   2.6488\n",
      "step: 61, loss_value: 3820.7935, content_loss: 25169.3320, style_loss:   2.6077\n",
      "step: 62, loss_value: 3921.3425, content_loss: 24620.7246, style_loss:   2.9185\n",
      "step: 63, loss_value: 3831.3020, content_loss: 24656.7051, style_loss:   2.7313\n",
      "step: 64, loss_value: 3947.8491, content_loss: 24630.1562, style_loss:   2.9697\n",
      "step: 65, loss_value: 3782.1306, content_loss: 24907.7070, style_loss:   2.5827\n",
      "step: 66, loss_value: 3790.5352, content_loss: 24807.3203, style_loss:   2.6196\n",
      "step: 67, loss_value: 3814.9014, content_loss: 24518.0742, style_loss:   2.7262\n",
      "step: 68, loss_value: 3736.1650, content_loss: 24302.8730, style_loss:   2.6118\n",
      "step: 69, loss_value: 3748.1218, content_loss: 23815.0410, style_loss:   2.7332\n",
      "step: 70, loss_value: 3664.9614, content_loss: 23642.4688, style_loss:   2.6014\n",
      "step: 71, loss_value: 3643.0659, content_loss: 23501.0000, style_loss:   2.5859\n",
      "step: 72, loss_value: 3593.7825, content_loss: 23329.7559, style_loss:   2.5216\n",
      "step: 73, loss_value: 3604.1821, content_loss: 22926.2148, style_loss:   2.6231\n",
      "step: 74, loss_value: 3559.1187, content_loss: 22775.2090, style_loss:   2.5632\n",
      "step: 75, loss_value: 3661.1089, content_loss: 22459.0938, style_loss:   2.8304\n",
      "step: 76, loss_value: 3615.8701, content_loss: 22725.4648, style_loss:   2.6866\n",
      "step: 77, loss_value: 3802.2834, content_loss: 22825.8184, style_loss:   3.0394\n",
      "step: 78, loss_value: 3628.3770, content_loss: 23210.5996, style_loss:   2.6146\n",
      "step: 79, loss_value: 3667.4106, content_loss: 23372.8320, style_loss:   2.6603\n",
      "step: 80, loss_value: 3676.2065, content_loss: 23361.8301, style_loss:   2.6800\n",
      "step: 81, loss_value: 3661.0891, content_loss: 23373.0879, style_loss:   2.6476\n",
      "step: 82, loss_value: 3771.6465, content_loss: 23162.1914, style_loss:   2.9109\n",
      "step: 83, loss_value: 3658.2290, content_loss: 23339.1445, style_loss:   2.6486\n",
      "step: 84, loss_value: 3672.8701, content_loss: 23480.0879, style_loss:   2.6497\n",
      "step: 85, loss_value: 3652.3113, content_loss: 23304.5762, style_loss:   2.6437\n",
      "step: 86, loss_value: 3689.1082, content_loss: 22877.6797, style_loss:   2.8027\n",
      "step: 87, loss_value: 3662.0925, content_loss: 22882.2695, style_loss:   2.7477\n",
      "step: 88, loss_value: 3631.8467, content_loss: 22992.4336, style_loss:   2.6652\n",
      "step: 89, loss_value: 3568.6511, content_loss: 23118.6152, style_loss:   2.5136\n",
      "step: 90, loss_value: 3522.2261, content_loss: 22781.0918, style_loss:   2.4882\n",
      "step: 91, loss_value: 3475.0264, content_loss: 22305.3574, style_loss:   2.4890\n",
      "step: 92, loss_value: 3469.1357, content_loss: 21830.4531, style_loss:   2.5722\n",
      "step: 93, loss_value: 3533.2344, content_loss: 21471.0371, style_loss:   2.7723\n",
      "step: 94, loss_value: 3645.5459, content_loss: 21522.4980, style_loss:   2.9866\n",
      "step: 95, loss_value: 3619.6248, content_loss: 21922.1602, style_loss:   2.8548\n",
      "step: 96, loss_value: 3547.6680, content_loss: 22553.3398, style_loss:   2.5847\n",
      "step: 97, loss_value: 3558.0186, content_loss: 22542.9590, style_loss:   2.6074\n",
      "step: 98, loss_value: 3504.3533, content_loss: 22356.6387, style_loss:   2.5374\n",
      "step: 99, loss_value: 3601.0972, content_loss: 21877.8203, style_loss:   2.8266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100, loss_value: 3505.0769, content_loss: 21978.5234, style_loss:   2.6144\n",
      "step: 101, loss_value: 3586.6157, content_loss: 21942.3867, style_loss:   2.7848\n",
      "step: 102, loss_value: 3454.3335, content_loss: 21948.7969, style_loss:   2.5189\n",
      "step: 103, loss_value: 3468.5681, content_loss: 21826.3223, style_loss:   2.5719\n",
      "step: 104, loss_value: 3466.2334, content_loss: 21838.4824, style_loss:   2.5648\n",
      "step: 105, loss_value: 3485.4004, content_loss: 21593.5410, style_loss:   2.6521\n",
      "step: 106, loss_value: 3545.5811, content_loss: 21269.9766, style_loss:   2.8372\n",
      "step: 107, loss_value: 3564.6567, content_loss: 21323.2207, style_loss:   2.8647\n",
      "step: 108, loss_value: 3464.9883, content_loss: 21767.2715, style_loss:   2.5765\n",
      "step: 109, loss_value: 3490.6843, content_loss: 21656.6855, style_loss:   2.6500\n",
      "step: 110, loss_value: 3419.1699, content_loss: 21622.3652, style_loss:   2.5139\n",
      "step: 111, loss_value: 3491.1445, content_loss: 21438.3516, style_loss:   2.6946\n",
      "step: 112, loss_value: 3421.1296, content_loss: 21408.7500, style_loss:   2.5605\n",
      "step: 113, loss_value: 3497.4885, content_loss: 21211.4844, style_loss:   2.7527\n",
      "step: 114, loss_value: 3446.5364, content_loss: 21107.8281, style_loss:   2.6715\n",
      "step: 115, loss_value: 3465.4321, content_loss: 21252.3516, style_loss:   2.6804\n",
      "step: 116, loss_value: 3536.1443, content_loss: 21151.9238, style_loss:   2.8419\n",
      "step: 117, loss_value: 3445.8599, content_loss: 21347.2207, style_loss:   2.6223\n",
      "step: 118, loss_value: 3457.8267, content_loss: 21307.9824, style_loss:   2.6541\n",
      "step: 119, loss_value: 3405.6240, content_loss: 21286.7852, style_loss:   2.5539\n",
      "step: 120, loss_value: 3360.2617, content_loss: 21251.4102, style_loss:   2.4702\n",
      "step: 121, loss_value: 3344.1565, content_loss: 21027.0117, style_loss:   2.4829\n",
      "step: 122, loss_value: 3323.6426, content_loss: 20582.1738, style_loss:   2.5308\n",
      "step: 123, loss_value: 3341.0298, content_loss: 20151.0762, style_loss:   2.6518\n",
      "step: 124, loss_value: 3439.0039, content_loss: 20075.8496, style_loss:   2.8628\n",
      "step: 125, loss_value: 3467.1021, content_loss: 20277.1602, style_loss:   2.8788\n",
      "step: 126, loss_value: 3381.8596, content_loss: 20928.2715, style_loss:   2.5781\n",
      "step: 127, loss_value: 3410.7739, content_loss: 21241.6738, style_loss:   2.5732\n",
      "step: 128, loss_value: 3339.0439, content_loss: 21215.4961, style_loss:   2.4350\n",
      "step: 129, loss_value: 3329.8521, content_loss: 20637.2344, style_loss:   2.5323\n",
      "step: 130, loss_value: 3262.9756, content_loss: 20289.0352, style_loss:   2.4681\n",
      "step: 131, loss_value: 3292.8545, content_loss: 19871.3535, style_loss:   2.6114\n",
      "step: 132, loss_value: 3245.1201, content_loss: 19729.1094, style_loss:   2.5444\n",
      "step: 133, loss_value: 3352.3594, content_loss: 19576.5703, style_loss:   2.7894\n",
      "step: 134, loss_value: 3251.1411, content_loss: 19747.6992, style_loss:   2.5527\n",
      "step: 135, loss_value: 3277.6062, content_loss: 19973.0371, style_loss:   2.5606\n",
      "step: 136, loss_value: 3353.4214, content_loss: 19852.6289, style_loss:   2.7363\n",
      "step: 137, loss_value: 3357.7847, content_loss: 19787.9551, style_loss:   2.7580\n",
      "step: 138, loss_value: 3563.4016, content_loss: 19760.5410, style_loss:   3.1747\n",
      "step: 139, loss_value: 3307.7612, content_loss: 20469.2988, style_loss:   2.5217\n",
      "step: 140, loss_value: 3369.6367, content_loss: 20946.5039, style_loss:   2.5500\n",
      "step: 141, loss_value: 3365.6018, content_loss: 21081.3613, style_loss:   2.5149\n",
      "step: 142, loss_value: 3395.2249, content_loss: 20814.6758, style_loss:   2.6275\n",
      "step: 143, loss_value: 3369.1152, content_loss: 20612.9570, style_loss:   2.6156\n",
      "step: 144, loss_value: 3310.1360, content_loss: 20551.1230, style_loss:   2.5100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "# VGG 自带的一个常量，之前VGG训练通过归一化，所以现在同样需要作此操作\n",
    "VGG_MEAN = [103.939, 116.779, 123.68] # rgb 三通道的均值\n",
    "\n",
    "class VGGNet():\n",
    "    '''\n",
    "    创建 vgg16 网络 结构\n",
    "    从模型中载入参数\n",
    "    '''\n",
    "    def __init__(self, data_dict):\n",
    "        '''\n",
    "        传入vgg16模型\n",
    "        :param data_dict: vgg16.npy (字典类型)\n",
    "        '''\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        '''\n",
    "        得到对应名称的卷积层\n",
    "        :param name: 卷积层名称\n",
    "        :return: 该卷积层输出\n",
    "        '''\n",
    "        return tf.constant(self.data_dict[name][0], name = 'conv')\n",
    "\n",
    "    def get_fc_weight(self, name):\n",
    "        '''\n",
    "        获得名字为name的全连接层权重\n",
    "        :param name: 连接层名称\n",
    "        :return: 该层权重\n",
    "        '''\n",
    "        return tf.constant(self.data_dict[name][0], name = 'fc')\n",
    "\n",
    "    def get_bias(self, name):\n",
    "        '''\n",
    "        获得名字为name的全连接层偏置\n",
    "        :param name: 连接层名称\n",
    "        :return: 该层偏置\n",
    "        '''\n",
    "        return tf.constant(self.data_dict[name][1], name = 'bias')\n",
    "\n",
    "\n",
    "    def conv_layer(self, x, name):\n",
    "        '''\n",
    "        创建一个卷积层\n",
    "        :param x:\n",
    "        :param name:\n",
    "        :return:\n",
    "        '''\n",
    "        # 在写计算图模型的时候，加一些必要的 name_scope，这是一个比较好的编程规范\n",
    "        # 可以防止命名冲突， 二可视化计算图的时候比较清楚\n",
    "        with tf.name_scope(name):\n",
    "            # 获得 w 和 b\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "\n",
    "            # 进行卷积计算\n",
    "            h = tf.nn.conv2d(x, conv_w, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            '''\n",
    "            因为此刻的 w 和 b 是从外部传递进来，所以使用 tf.nn.conv2d()\n",
    "            tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu = None, name = None) 参数说明：\n",
    "            input 输入的tensor， 格式[batch, height, width, channel]\n",
    "            filter 卷积核 [filter_height, filter_width, in_channels, out_channels] \n",
    "                分别是：卷积核高，卷积核宽，输入通道数，输出通道数\n",
    "            strides 步长 卷积时在图像每一维度的步长，长度为4\n",
    "            padding 参数可选择 “SAME” “VALID”\n",
    "            \n",
    "            '''\n",
    "            # 加上偏置\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            # 使用激活函数\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "\n",
    "\n",
    "    def pooling_layer(self, x, name):\n",
    "        '''\n",
    "        创建池化层\n",
    "        :param x: 输入的tensor\n",
    "        :param name: 池化层名称\n",
    "        :return: tensor\n",
    "        '''\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize = [1, 2, 2, 1], # 核参数， 注意：都是4维\n",
    "                              strides = [1, 2, 2, 1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name\n",
    "                              )\n",
    "\n",
    "    def fc_layer(self, x, name, activation = tf.nn.relu):\n",
    "        '''\n",
    "        创建全连接层\n",
    "        :param x: 输入tensor\n",
    "        :param name: 全连接层名称\n",
    "        :param activation: 激活函数名称\n",
    "        :return: 输出tensor\n",
    "        '''\n",
    "        with tf.name_scope(name, activation):\n",
    "            # 获取全连接层的 w 和 b\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            # 矩阵相乘 计算\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            #　添加偏置\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            # 因为最后一层是没有激活函数ｒｅｌｕ的，所以在此要做出判断\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "\n",
    "    def flatten_layer(self, x, name):\n",
    "        '''\n",
    "        展平\n",
    "        :param x: input_tensor\n",
    "        :param name:\n",
    "        :return: 二维矩阵\n",
    "        '''\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            # 计算后三维合并后的大小\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            # 形成一个二维矩阵\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x\n",
    "\n",
    "    def build(self, x_rgb):\n",
    "        '''\n",
    "        创建vgg16 网络\n",
    "        :param x_rgb: [1, 224, 224, 3]\n",
    "        :return:\n",
    "        '''\n",
    "        start_time = time.time()\n",
    "        print('模型开始创建……')\n",
    "        # 将输入图像进行处理，将每个通道减去均值\n",
    "        r, g, b = tf.split(x_rgb, [1, 1, 1], axis = 3)\n",
    "        '''\n",
    "        tf.split(value, num_or_size_split, axis=0)用法：\n",
    "        value:输入的Tensor\n",
    "        num_or_size_split:有两种用法：\n",
    "            1.直接传入一个整数，代表会被切成几个张量，切割的维度有axis指定\n",
    "            2.传入一个向量，向量长度就是被切的份数。传入向量的好处在于，可以指定每一份有多少元素\n",
    "        axis, 指定从哪一个维度切割\n",
    "        因此，上一句的意思就是从第4维切分，分为3份，每一份只有1个元素\n",
    "        '''\n",
    "        # 将 处理后的通道再次合并起来\n",
    "        x_bgr = tf.concat([b - VGG_MEAN[0], g - VGG_MEAN[1], r - VGG_MEAN[2]], axis = 3)\n",
    "\n",
    "#        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "\n",
    "        # 开始构建卷积层\n",
    "        # vgg16 的网络结构\n",
    "        # 第一层：2个卷积层 1个pooling层\n",
    "        # 第二层：2个卷积层 1个pooling层\n",
    "        # 第三层：3个卷积层 1个pooling层\n",
    "        # 第四层：3个卷积层 1个pooling层\n",
    "        # 第五层：3个卷积层 1个pooling层\n",
    "        # 第六层： 全连接\n",
    "        # 第七层： 全连接\n",
    "        # 第八层： 全连接\n",
    "\n",
    "        # 这些变量名称不能乱取，必须要和vgg16模型保持一致\n",
    "        # 另外，将这些卷积层用self.的形式，方便以后取用方便\n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "\n",
    "        ''' 因为风格转换只需要 卷积层  的数据\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation = None)\n",
    "        self.prob = tf.nn.softmax(self.fc8, name = 'prob')\n",
    "        '''\n",
    "\n",
    "\n",
    "        print('创建模型结束：%4ds' % (time.time() - start_time))\n",
    "\n",
    "# 指定 model 路径\n",
    "vgg16_npy_pyth = './vgg16.npy'\n",
    "# 内容图像 路径\n",
    "content_img_path = './source_images/2.jpg'\n",
    "# 风格图像路径\n",
    "style_img_path = './source_images/1.jpg'\n",
    "\n",
    "# 训练的步数\n",
    "num_steps = 100\n",
    "# 指定学习率\n",
    "learning_rate = 10\n",
    "\n",
    "# 设置 两个 参数\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "# 输入 目录\n",
    "output_dir = './run_style_transfer'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "def initial_result(shape, mean, stddev):\n",
    "    '''\n",
    "    定义一个初始化好的随机图片，然后在该图片上不停的梯度下降来得到效果。\n",
    "    :param shape: 输入形状\n",
    "    :param mean: 均值\n",
    "    :param stddev: 方法\n",
    "    :return: 图片\n",
    "    '''\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev) # 一个截断的正态分布\n",
    "    '''\n",
    "    tf.truncated_normal(shape, mean, stddev) 生成截断的生态分布函数\n",
    "    如果产生的正态分布值和均值差值大于二倍的标准差，那就重新生成。\n",
    "    '''\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    '''\n",
    "    读取图片\n",
    "    :param img_name: 图片路径\n",
    "    :return: 4维矩阵\n",
    "    '''\n",
    "    img = Image.open(img_name)\n",
    "    # 图像为三通道（224， 244， 3），但是需要转化为4维\n",
    "    np_img = np.array(img) # 224, 224, 3\n",
    "    np_img = np.asarray([np_img], dtype = np.int32) # 这个函数作用不太理解 (1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    '''\n",
    "    计算 gram 矩阵\n",
    "    :param x: 特征图，shape：[1, width, height, channel]\n",
    "    :return:\n",
    "    '''\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    # 这里求出来的是 每一个feature map之间的相似度\n",
    "    features = tf.reshape(x, [b, h * w, ch]) # 将二三维的维度合并，已组成三维\n",
    "    # 相似度矩阵 方法： 将矩阵转置为[ch, b*w], 再乘原矩阵，最后的矩阵是[ch , ch]\n",
    "    # 防止矩阵数值过大，除以一个常数\n",
    "    gram = tf.matmul(features, features, adjoint_a = True) / tf.constant(ch * w * h, tf.float32) # 参数3， 表示将第一个参数转置\n",
    "    return gram\n",
    "\n",
    "\n",
    "# 生成一个图像，均值为127.5，方差为20\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "\n",
    "# 读取 内容图像 和 风格图像\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape = [1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape = [1, 224, 224, 3])\n",
    "\n",
    "# 载入模型， 注意：在python3中，需要添加一句： encoding='latin1'\n",
    "data_dict = np.load(vgg16_npy_pyth, encoding='latin1',allow_pickle=True).item()\n",
    "\n",
    "\n",
    "# 创建这三张图像的 vgg 对象\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "# 创建 每个 神经网络\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "# 提取哪些层特征\n",
    "# 需要注意的是：内容特征抽取的层数和结果特征抽取的层数必须相同\n",
    "# 风格特征抽取的层数和结果特征抽取的层数必须相同\n",
    "content_features = [vgg_for_content.conv1_2,\n",
    "                    vgg_for_content.conv2_2,\n",
    "                    # vgg_for_content.conv3_3,\n",
    "                    # vgg_for_content.conv4_3,\n",
    "                    # vgg_for_content.conv5_3,\n",
    "                    ]\n",
    "\n",
    "result_content_features = [vgg_for_result.conv1_2,\n",
    "                          vgg_for_result.conv2_2,\n",
    "                          # vgg_for_result.conv3_3,\n",
    "                          # vgg_for_result.conv4_3,\n",
    "                          # vgg_for_result.conv5_3\n",
    "                          ]\n",
    "\n",
    "# feature_size, [1, width, height, channel]\n",
    "style_features = [# vgg_for_style.conv1_2,\n",
    "                          # vgg_for_style.conv2_2,\n",
    "                          # vgg_for_style.conv3_3,\n",
    "                          vgg_for_style.conv4_3,\n",
    "                          # vgg_for_style.conv5_3\n",
    "                          ]\n",
    "\n",
    "# 为列表中每一个元素，都计算 gram\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [# vgg_for_result.conv1_2,\n",
    "                          # vgg_for_result.conv2_2,\n",
    "                          # vgg_for_result.conv3_3,\n",
    "                          vgg_for_result.conv4_3,\n",
    "                          # vgg_for_result.conv5_3\n",
    "                          ]\n",
    "\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# 计算内容损失\n",
    "# 卷积层的形状 shape:[1, width, height, channel], 需要在三个通道上做平均\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_)**2, axis = [1, 2, 3])\n",
    "\n",
    "# 风格内容损失\n",
    "\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    # 因为在计算gram矩阵的时候，降低了一维，所以，只需要在[1, 2]两个维度求均值即可\n",
    "    style_loss += tf.reduce_mean( (s - s_)** 2, [1, 2] )\n",
    "\n",
    "\n",
    "# 总的损失函数\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "\n",
    "\n",
    "train_op = tf.train.AdamOptimizer( learning_rate ).minimize(loss)\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ = \\\n",
    "            sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                         content:content_val,\n",
    "                         style:style_val\n",
    "                     })\n",
    "        # 因为loss_value等，是一个数组，需要通过索引将值去出\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' % (step+1,\n",
    "                                                                  loss_value[0],\n",
    "                                                                  content_loss_value[0],\n",
    "                                                                  style_loss_value[0]))\n",
    "        result_img_path = os.path.join(output_dir, 'result_%05d.jpg'%(step+1))\n",
    "        result_val = result.eval(sess)[0] # 将图像取出，因为之前是4维，所以需要使用一个索引0，将其取出\n",
    "\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        # np.clip() numpy.clip(a, a_min, a_max, out=None)[source]\n",
    "        # 其中a是一个数组，后面两个参数分别表示最小和最大值\n",
    "\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        # 保存图像\n",
    "        img.save(result_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
