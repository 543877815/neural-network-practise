{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batches.meta', 'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'readme.html', 'test_batch']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "CIFAR_DIR = './cifar-10-batches-py'\n",
    "print(os.listdir(CIFAR_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (50000,)\n",
      "(10000, 3072) (10000,)\n",
      "[[-0.42745098 -0.24705882 -0.34901961 ... -0.30196078 -0.33333333\n",
      "  -0.35686275]\n",
      " [-0.27058824 -0.28627451 -0.44313725 ... -0.15294118  0.00392157\n",
      "   0.00392157]\n",
      " [ 0.62352941  0.65490196  0.6627451  ...  0.81176471  0.81176471\n",
      "   0.80392157]\n",
      " ...\n",
      " [-0.36470588 -0.19215686 -0.05882353 ... -0.34117647 -0.8745098\n",
      "  -0.86666667]\n",
      " [ 0.17647059 -0.19215686  0.18431373 ...  0.05882353  0.01960784\n",
      "   0.08235294]\n",
      " [ 0.02745098  0.16078431  0.1372549  ...  0.00392157  0.02745098\n",
      "   0.01960784]] [6 9 9 7 2 3 0 5 9 8]\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"read data from data file.\"\"\"\n",
    "    with open (filename, \"rb\") as f:\n",
    "        data = pickle.load(f,encoding='bytes')\n",
    "        return data[b'data'], data[b'labels']\n",
    "    \n",
    "class CifarData:\n",
    "    def __init__(self, filenames, need_shuffle):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        for filename in filenames:\n",
    "            data, labels = load_data(filename)\n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)\n",
    "        self._data = np.vstack(all_data)\n",
    "        self._data = self._data / 127.5 -1 # 归一化\n",
    "        self._labels = np.hstack(all_labels)\n",
    "        print(self._data.shape, self._labels.shape)\n",
    "        self._num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0 ## 遍历起始位置\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "    \n",
    "    def _shuffle_data(self):\n",
    "        # [0, 1, 2, 3, 4, 5] -> [5, 3, 2, 4, 0 ,1]\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0 \n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more example\")\n",
    "        \n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator: end_indicator]    \n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "train_filenames = [os.path.join(CIFAR_DIR, 'data_batch_%d' % i) for i in range(1,6)]\n",
    "test_filenames = [os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "train_data = CifarData(train_filenames, True)\n",
    "test_data = CifarData(test_filenames, False)\n",
    "\n",
    "batch_data, batch_labels = train_data.next_batch(10)\n",
    "print(batch_data, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 3072])\n",
    "# [None], eg: [0,6,5,3]\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# (3072, 10)\n",
    "w = tf.get_variable('w', [x.get_shape()[-1], 10],\n",
    "                   initializer=tf.random_normal_initializer(0, 1))\n",
    "\n",
    "# (10, )\n",
    "b = tf.get_variable('b', [10],\n",
    "                   initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "# [None, 3072] * [3072, 10] = [None, 10]\n",
    "y_ = tf.matmul(x, w) + b\n",
    "\n",
    "# mean square loss\n",
    "\"\"\"\n",
    "# course: 1 + e^x\n",
    "# api: e^x / sum(e^x)\n",
    "# [[0.01, 0.9, ..., 0.03], []]\n",
    "p_y = tf.nn.softmax(y_)\n",
    "# 5 -> [0,0,0,0,1,0,0,0,0,0]\n",
    "y_one_hot = tf.one_hot(y, 10, dtype=tf.float32)\n",
    "loss = tf.reduce_mean(tf.square(y_one_hot - p_y))\n",
    "\"\"\"\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y,logits=y_)\n",
    "# y_ -> softmax\n",
    "# y -> one_hot\n",
    "#loss = ylog_\n",
    "\n",
    "# indices\n",
    "predict = tf.argmax(y_, 1)\n",
    "# [1,0,1,0]\n",
    "correct_prediction = tf.equal(predict, y)\n",
    "accuracy= tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] step: 0, loss: 0.18556, acc:0.05000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 1, acc: 0.09650\n",
      "[Train] step: 500, loss: 0.17540, acc:0.10000\n",
      "[Train] step: 1000, loss: 0.16844, acc:0.15000\n",
      "[Train] step: 1500, loss: 0.13436, acc:0.30000\n",
      "[Train] step: 2000, loss: 0.15305, acc:0.20000\n",
      "[Train] step: 2500, loss: 0.14513, acc:0.25000\n",
      "[Train] step: 3000, loss: 0.13234, acc:0.30000\n",
      "[Train] step: 3500, loss: 0.12000, acc:0.40000\n",
      "[Train] step: 4000, loss: 0.13236, acc:0.30000\n",
      "[Train] step: 4500, loss: 0.13468, acc:0.30000\n",
      "[Train] step: 5000, loss: 0.12586, acc:0.35000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 5001, acc: 0.29050\n",
      "[Train] step: 5500, loss: 0.09568, acc:0.50000\n",
      "[Train] step: 6000, loss: 0.11052, acc:0.45000\n",
      "[Train] step: 6500, loss: 0.16871, acc:0.15000\n",
      "[Train] step: 7000, loss: 0.16604, acc:0.15000\n",
      "[Train] step: 7500, loss: 0.12365, acc:0.35000\n",
      "[Train] step: 8000, loss: 0.10866, acc:0.45000\n",
      "[Train] step: 8500, loss: 0.08986, acc:0.55000\n",
      "[Train] step: 9000, loss: 0.13758, acc:0.30000\n",
      "[Train] step: 9500, loss: 0.12998, acc:0.35000\n",
      "[Train] step: 10000, loss: 0.14474, acc:0.30000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 10001, acc: 0.33600\n",
      "[Train] step: 10500, loss: 0.08907, acc:0.50000\n",
      "[Train] step: 11000, loss: 0.11309, acc:0.40000\n",
      "[Train] step: 11500, loss: 0.10389, acc:0.45000\n",
      "[Train] step: 12000, loss: 0.13780, acc:0.30000\n",
      "[Train] step: 12500, loss: 0.15416, acc:0.20000\n",
      "[Train] step: 13000, loss: 0.07852, acc:0.60000\n",
      "[Train] step: 13500, loss: 0.16636, acc:0.15000\n",
      "[Train] step: 14000, loss: 0.11959, acc:0.40000\n",
      "[Train] step: 14500, loss: 0.12013, acc:0.40000\n",
      "[Train] step: 15000, loss: 0.13721, acc:0.30000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 15001, acc: 0.34550\n",
      "[Train] step: 15500, loss: 0.12105, acc:0.35000\n",
      "[Train] step: 16000, loss: 0.15413, acc:0.20000\n",
      "[Train] step: 16500, loss: 0.16046, acc:0.15000\n",
      "[Train] step: 17000, loss: 0.12159, acc:0.40000\n",
      "[Train] step: 17500, loss: 0.07957, acc:0.60000\n",
      "[Train] step: 18000, loss: 0.12760, acc:0.35000\n",
      "[Train] step: 18500, loss: 0.09979, acc:0.50000\n",
      "[Train] step: 19000, loss: 0.12563, acc:0.35000\n",
      "[Train] step: 19500, loss: 0.12182, acc:0.35000\n",
      "[Train] step: 20000, loss: 0.10167, acc:0.45000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 20001, acc: 0.35100\n",
      "[Train] step: 20500, loss: 0.15748, acc:0.20000\n",
      "[Train] step: 21000, loss: 0.09976, acc:0.50000\n",
      "[Train] step: 21500, loss: 0.10998, acc:0.45000\n",
      "[Train] step: 22000, loss: 0.08609, acc:0.55000\n",
      "[Train] step: 22500, loss: 0.11403, acc:0.40000\n",
      "[Train] step: 23000, loss: 0.12776, acc:0.35000\n",
      "[Train] step: 23500, loss: 0.12679, acc:0.35000\n",
      "[Train] step: 24000, loss: 0.10154, acc:0.45000\n",
      "[Train] step: 24500, loss: 0.11676, acc:0.40000\n",
      "[Train] step: 25000, loss: 0.11905, acc:0.40000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 25001, acc: 0.36200\n",
      "[Train] step: 25500, loss: 0.11566, acc:0.40000\n",
      "[Train] step: 26000, loss: 0.12101, acc:0.40000\n",
      "[Train] step: 26500, loss: 0.08571, acc:0.55000\n",
      "[Train] step: 27000, loss: 0.09796, acc:0.50000\n",
      "[Train] step: 27500, loss: 0.12965, acc:0.35000\n",
      "[Train] step: 28000, loss: 0.14903, acc:0.25000\n",
      "[Train] step: 28500, loss: 0.11270, acc:0.40000\n",
      "[Train] step: 29000, loss: 0.12072, acc:0.40000\n",
      "[Train] step: 29500, loss: 0.10835, acc:0.45000\n",
      "[Train] step: 30000, loss: 0.13754, acc:0.30000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 30001, acc: 0.36800\n",
      "[Train] step: 30500, loss: 0.13871, acc:0.25000\n",
      "[Train] step: 31000, loss: 0.10785, acc:0.45000\n",
      "[Train] step: 31500, loss: 0.10998, acc:0.45000\n",
      "[Train] step: 32000, loss: 0.11434, acc:0.40000\n",
      "[Train] step: 32500, loss: 0.06387, acc:0.65000\n",
      "[Train] step: 33000, loss: 0.10802, acc:0.45000\n",
      "[Train] step: 33500, loss: 0.15000, acc:0.25000\n",
      "[Train] step: 34000, loss: 0.07000, acc:0.65000\n",
      "[Train] step: 34500, loss: 0.12269, acc:0.35000\n",
      "[Train] step: 35000, loss: 0.11861, acc:0.40000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 35001, acc: 0.37000\n",
      "[Train] step: 35500, loss: 0.12584, acc:0.35000\n",
      "[Train] step: 36000, loss: 0.11503, acc:0.40000\n",
      "[Train] step: 36500, loss: 0.12002, acc:0.40000\n",
      "[Train] step: 37000, loss: 0.12476, acc:0.35000\n",
      "[Train] step: 37500, loss: 0.14677, acc:0.25000\n",
      "[Train] step: 38000, loss: 0.14500, acc:0.25000\n",
      "[Train] step: 38500, loss: 0.10990, acc:0.45000\n",
      "[Train] step: 39000, loss: 0.10981, acc:0.45000\n",
      "[Train] step: 39500, loss: 0.15634, acc:0.20000\n",
      "[Train] step: 40000, loss: 0.09465, acc:0.50000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 40001, acc: 0.37350\n",
      "[Train] step: 40500, loss: 0.11998, acc:0.40000\n",
      "[Train] step: 41000, loss: 0.12812, acc:0.35000\n",
      "[Train] step: 41500, loss: 0.13756, acc:0.30000\n",
      "[Train] step: 42000, loss: 0.11564, acc:0.40000\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 20\n",
    "train_steps = 100000\n",
    "test_steps =100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(train_steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "        loss_val, accu_val, _ = sess.run(\n",
    "            [loss, accuracy, train_op], \n",
    "            feed_dict={\n",
    "                x: batch_data,\n",
    "                y: batch_labels})\n",
    "        if i % 500 == 0:\n",
    "            print('[Train] step: %d, loss: %4.5f, acc:%4.5f'% (i, loss_val, accu_val)) \n",
    "        if i % 5000 == 0:\n",
    "            test_data = CifarData(test_filenames, False)\n",
    "            \n",
    "            all_test_acc_val = []\n",
    "            \n",
    "            for j in range(test_steps):\n",
    "                test_batch_data, test_batch_labels = test_data.next_batch(batch_size)\n",
    "                test_acc_val = sess.run([accuracy],\n",
    "                                       feed_dict={\n",
    "                                           x: test_batch_data,\n",
    "                                           y: test_batch_labels\n",
    "                                       })\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc = np.mean(all_test_acc_val)\n",
    "            print('[Test] Step: %d, acc: %4.5f'%(i+1, test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
