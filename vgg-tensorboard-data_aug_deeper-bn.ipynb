{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batches.meta', 'test_batch', 'data_batch_3', 'data_batch_2', 'data_batch_4', 'data_batch_1', 'readme.html', 'data_batch_5']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "CIFAR_DIR = './cifar-10-batches-py'\n",
    "print(os.listdir(CIFAR_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "# 1. 指定面板图上显示的变量\n",
    "# 2. 训练过程中将这些变量计算出来，输出到文件中\n",
    "# 3. 文件解析 ./tensorboard --logdir=dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (50000,)\n",
      "(10000, 3072) (10000,)\n",
      "[[102 101  98 ...  69  68  66]\n",
      " [ 62  81  83 ... 190 190 196]\n",
      " [130 102 102 ... 103 102  98]\n",
      " ...\n",
      " [ 13  13  13 ... 125 108 113]\n",
      " [187 185 183 ... 214 180 162]\n",
      " [123 119 107 ... 143 134 138]] [7 7 8 8 9 0 1 6 0 6]\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"read data from data file.\"\"\"\n",
    "    with open (filename, \"rb\") as f:\n",
    "        data = pickle.load(f,encoding='bytes')\n",
    "        return data[b'data'], data[b'labels']\n",
    "    \n",
    "class CifarData:\n",
    "    def __init__(self, filenames, need_shuffle):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        for filename in filenames:\n",
    "            data, labels = load_data(filename)\n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)\n",
    "        self._data = np.vstack(all_data)\n",
    "        self._data = self._data\n",
    "        self._labels = np.hstack(all_labels)\n",
    "        print(self._data.shape, self._labels.shape)\n",
    "        self._num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0 ## 遍历起始位置\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "    \n",
    "    def _shuffle_data(self):\n",
    "        # [0, 1, 2, 3, 4, 5] -> [5, 3, 2, 4, 0 ,1]\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0 \n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more example\")\n",
    "        \n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator: end_indicator]    \n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "train_filenames = [os.path.join(CIFAR_DIR, 'data_batch_%d' % i) for i in range(1,6)]\n",
    "test_filenames = [os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "train_data = CifarData(train_filenames, True)\n",
    "test_data = CifarData(test_filenames, False)\n",
    "\n",
    "batch_data, batch_labels = train_data.next_batch(10)\n",
    "print(batch_data, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0804 08:16:43.163650 139821838079808 deprecation.py:323] From <ipython-input-4-4ee01a49d81b>:39: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0804 08:16:43.167143 139821838079808 deprecation.py:506] From /root/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0804 08:16:43.323326 139821838079808 deprecation.py:323] From <ipython-input-4-4ee01a49d81b>:40: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W0804 08:16:43.502018 139821838079808 deprecation.py:323] From <ipython-input-4-4ee01a49d81b>:48: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0804 08:16:43.910323 139821838079808 deprecation.py:323] From <ipython-input-4-4ee01a49d81b>:66: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0804 08:16:44.066340 139821838079808 deprecation.py:323] From <ipython-input-4-4ee01a49d81b>:67: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0804 08:16:44.347496 139821838079808 deprecation.py:323] From /root/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size, 3072])\n",
    "\n",
    "# [None], eg: [0,6,5,3]\n",
    "y = tf.placeholder(tf.int64, [batch_size])\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 3, 32, 32])\n",
    "# 32*32\n",
    "x_image = tf.transpose(x_image, perm=[0, 2, 3, 1])\n",
    "\n",
    "x_image_arr = tf.split(x_image, num_or_size_splits=batch_size, axis=0)\n",
    "result_x_image_arr = []\n",
    "\n",
    "for x_single_image in x_image_arr:\n",
    "    # x_single_image: [1, 32, 32, 3] -> [32, 32, 3]\n",
    "    x_single_image = tf.reshape(x_single_image, [32, 32, 3])\n",
    "    data_aug_1 = tf.image.random_flip_left_right(x_single_image)\n",
    "    data_aug_2 = tf.image.random_brightness(data_aug_1, max_delta=65)\n",
    "    data_aug_3 = tf.image.random_contrast(data_aug_2, lower=0.2, upper=1.8)\n",
    "    x_single_image = tf.reshape(data_aug_3, [1, 32, 32, 3])\n",
    "    result_x_image_arr.append(x_single_image)\n",
    "result_x_images = tf.concat(result_x_image_arr, axis=0)\n",
    "normal_result_x_images = result_x_images / 127.5 - 1\n",
    "    \n",
    "    \n",
    "def conv_wrapper(inputs, name, is_training, output_channel=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu):\n",
    "    \"\"\"wrapper of tf.layers.conv2d\"\"\"\n",
    "    # without bn: conv -> activation\n",
    "    # with batch normalization: conv -> bn -> activation\n",
    "    with tf.name_scope(name):\n",
    "        conv2d = tf.layers.conv2d(inputs,\n",
    "                               output_channel,\n",
    "                               kernel_size,\n",
    "                               padding=padding,\n",
    "                               activation=None,\n",
    "                               name=name + 'conv12d')\n",
    "        bn = tf.layers.batch_normalization(conv2d, training = is_training)\n",
    "        return activation(bn)\n",
    "    \n",
    "def pooling_wrapper(inputs, name):\n",
    "    \"\"\"wrapper of tf.layers.conv2d\"\"\"\n",
    "    return tf.layers.max_pooling2d(inputs,\n",
    "                                  (2,2),\n",
    "                                  (2,2),\n",
    "                                  name=name)\n",
    "\n",
    "conv1_1 = conv_wrapper(normal_result_x_images, 'conv1_1', is_training)\n",
    "conv1_2 = conv_wrapper(conv1_1, 'conv1_2', is_training)\n",
    "conv1_3 = conv_wrapper(conv1_2, 'conv1_3', is_training)\n",
    "pooling1 = pooling_wrapper(conv1_3, 'pool1')\n",
    "\n",
    "conv2_1 = conv_wrapper(pooling1, 'conv2_1', is_training)\n",
    "conv2_2 = conv_wrapper(conv2_1, 'conv2_2', is_training)\n",
    "conv2_3 = conv_wrapper(conv2_2, 'conv2_3', is_training)\n",
    "pooling2 = pooling_wrapper(conv2_3, 'pool2')\n",
    "\n",
    "conv3_1 = conv_wrapper(pooling2, 'conv3_1', is_training)\n",
    "conv3_2 = conv_wrapper(conv3_1, 'conv3_2', is_training)\n",
    "conv3_3 = conv_wrapper(conv3_2, 'conv3_3', is_training)\n",
    "pooling3 = pooling_wrapper(conv3_3, 'pool3')\n",
    "\n",
    "# [None, 4 * 4 * 32]\n",
    "flatten = tf.layers.flatten(pooling3)\n",
    "y_ = tf.layers.dense(flatten, 10)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y,logits=y_)\n",
    "# y_ -> softmax\n",
    "# y -> one_hot\n",
    "#loss = ylog_\n",
    "\n",
    "# indices\n",
    "predict = tf.argmax(y_, 1)\n",
    "# [1,0,1,0]\n",
    "correct_prediction = tf.equal(predict, y)\n",
    "accuracy= tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summary(var, name):\n",
    "    \"\"\"Constructs summary fro statistics of a variable\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "    variable_summary(conv1_1, 'conv1_1')\n",
    "    variable_summary(conv1_2, 'conv1_2')\n",
    "    variable_summary(conv2_1, 'conv2_1')\n",
    "    variable_summary(conv2_2, 'conv2_2')\n",
    "    variable_summary(conv3_1, 'conv3_1')\n",
    "    variable_summary(conv3_2, 'conv3_2')\n",
    "        \n",
    "# 1. 指定面板图上显示的变量\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "# 'loss': <10, 1.1>, <20, 1.08>\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "inputs_summary = tf.summary.image('inputs_image', result_x_images)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "merged_summary_test = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "\n",
    "# 2. 训练过程中将这些变量计算出来，输出到文件中\n",
    "LOG_DIR = '.'\n",
    "run_label = 'run_vgg_tensorboard'\n",
    "run_dir = os.path.join(LOG_DIR, run_label)\n",
    "if not os.path.exists(run_dir):\n",
    "    os.mkdir(run_dir)\n",
    "train_log_dir = os.path.join(run_dir, 'train')\n",
    "test_log_dir = os.path.join(run_dir, 'test')\n",
    "\n",
    "if not os.path.exists(train_log_dir):\n",
    "    os.mkdir(train_log_dir)\n",
    "if not os.path.exists(test_log_dir):\n",
    "    os.mkdir(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] step: 0, loss: 3.89898, acc:0.00000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 1, acc: 0.09900\n",
      "[Train] step: 100, loss: 1.74928, acc:0.30000\n",
      "[Train] step: 200, loss: 2.35872, acc:0.15000\n",
      "[Train] step: 300, loss: 1.62523, acc:0.50000\n",
      "[Train] step: 400, loss: 1.67639, acc:0.35000\n",
      "[Train] step: 500, loss: 1.57177, acc:0.35000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 501, acc: 0.11800\n",
      "[Train] step: 600, loss: 2.03720, acc:0.25000\n",
      "[Train] step: 700, loss: 1.87319, acc:0.30000\n",
      "[Train] step: 800, loss: 1.79665, acc:0.25000\n",
      "[Train] step: 900, loss: 2.13575, acc:0.35000\n",
      "[Train] step: 1000, loss: 1.13697, acc:0.60000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 1001, acc: 0.25250\n",
      "[Train] step: 1100, loss: 1.53508, acc:0.40000\n",
      "[Train] step: 1200, loss: 1.16382, acc:0.60000\n",
      "[Train] step: 1300, loss: 1.71808, acc:0.45000\n",
      "[Train] step: 1400, loss: 1.32305, acc:0.60000\n",
      "[Train] step: 1500, loss: 1.90534, acc:0.35000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 1501, acc: 0.20850\n",
      "[Train] step: 1600, loss: 1.43253, acc:0.50000\n",
      "[Train] step: 1700, loss: 1.64305, acc:0.50000\n",
      "[Train] step: 1800, loss: 1.30034, acc:0.50000\n",
      "[Train] step: 1900, loss: 1.07132, acc:0.60000\n",
      "[Train] step: 2000, loss: 1.23754, acc:0.55000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 2001, acc: 0.22850\n",
      "[Train] step: 2100, loss: 1.37488, acc:0.40000\n",
      "[Train] step: 2200, loss: 1.14794, acc:0.70000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ad6cdeb2299d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 is_training: True})\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccu_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_ops_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_output_summary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 20\n",
    "train_steps = 100000\n",
    "test_steps =100\n",
    "\n",
    "output_summary_every_steps = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # 2. 训练过程中将这些变量计算出来，输出到文件中\n",
    "    train_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(test_log_dir)\n",
    "    \n",
    "    fixed_test_batch_data, fixed_test_batch_labels = train_data.next_batch(batch_size)\n",
    "    for i in range(train_steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "        eval_ops = [loss, accuracy, train_op]\n",
    "        should_output_summary = ((i+1) % output_summary_every_steps == 0)\n",
    "        if should_output_summary:\n",
    "            eval_ops.append(merged_summary)\n",
    "        \n",
    "        eval_ops_results = sess.run(\n",
    "            eval_ops, \n",
    "            feed_dict={\n",
    "                x: batch_data,\n",
    "                y: batch_labels,\n",
    "                is_training: True})\n",
    "        loss_val, accu_val = eval_ops_results[0:2]\n",
    "        if should_output_summary:\n",
    "            train_summary_str = eval_ops_results[-1]\n",
    "            train_writer.add_summary(train_summary_str, i+1)\n",
    "            test_summary_str = sess.run([merged_summary_test],\n",
    "                                           feed_dict={\n",
    "                                               x: fixed_test_batch_data,\n",
    "                                               y: fixed_test_batch_labels,\n",
    "                                               is_training: False\n",
    "                                           })[0]\n",
    "            test_writer.add_summary(test_summary_str, i+1)\n",
    "        if i % 100 == 0:\n",
    "            print('[Train] step: %d, loss: %4.5f, acc:%4.5f'% (i, loss_val, accu_val)) \n",
    "        if i % 500 == 0:\n",
    "            test_data = CifarData(test_filenames, False)\n",
    "            \n",
    "            all_test_acc_val = []\n",
    "            \n",
    "            for j in range(test_steps):\n",
    "                test_batch_data, test_batch_labels = test_data.next_batch(batch_size)\n",
    "                test_acc_val = sess.run([accuracy],\n",
    "                                       feed_dict={\n",
    "                                           x: test_batch_data,\n",
    "                                           y: test_batch_labels,\n",
    "                                           is_training: False\n",
    "                                       })\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc = np.mean(all_test_acc_val)\n",
    "            print('[Test] Step: %d, acc: %4.5f'%(i+1, test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[Test] Step: %d, acc: %4.5f'%(i+1, test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 命令\n",
    "# tensorboard --logdir=train:'train',test:'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0804 08:20:48.378831 140093414913856 deprecation.py:323] From <ipython-input-1-7019a39229a4>:183: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0804 08:20:48.382457 140093414913856 deprecation.py:506] From /root/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0804 08:20:48.541692 140093414913856 deprecation.py:323] From <ipython-input-1-7019a39229a4>:191: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W0804 08:20:48.723736 140093414913856 deprecation.py:323] From <ipython-input-1-7019a39229a4>:207: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0804 08:20:49.495924 140093414913856 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0804 08:20:49.497012 140093414913856 deprecation.py:323] From /root/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0804 08:20:49.654558 140093414913856 deprecation.py:323] From <ipython-input-1-7019a39229a4>:229: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0804 08:20:49.875899 140093414913856 deprecation.py:323] From /root/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 500, loss: 1.95358, acc: 0.20000\n",
      "[Train] Step: 1000, loss: 1.53150, acc: 0.50000\n",
      "[Train] Step: 1500, loss: 1.49144, acc: 0.50000\n",
      "[Train] Step: 2000, loss: 1.43633, acc: 0.50000\n",
      "[Train] Step: 2500, loss: 0.84609, acc: 0.75000\n",
      "[Train] Step: 3000, loss: 1.08949, acc: 0.55000\n",
      "[Train] Step: 3500, loss: 0.79712, acc: 0.70000\n",
      "[Train] Step: 4000, loss: 1.09326, acc: 0.65000\n",
      "[Train] Step: 4500, loss: 1.09979, acc: 0.65000\n",
      "[Train] Step: 5000, loss: 1.05461, acc: 0.70000\n",
      "[Test ] Step: 5000, acc: 0.69350\n",
      "[Train] Step: 5500, loss: 0.47227, acc: 0.80000\n",
      "[Train] Step: 6000, loss: 1.26132, acc: 0.55000\n",
      "[Train] Step: 6500, loss: 0.73153, acc: 0.80000\n",
      "[Train] Step: 7000, loss: 0.47675, acc: 0.85000\n",
      "[Train] Step: 7500, loss: 0.55175, acc: 0.80000\n",
      "[Train] Step: 8000, loss: 1.11786, acc: 0.65000\n",
      "[Train] Step: 8500, loss: 0.83062, acc: 0.70000\n",
      "[Train] Step: 9000, loss: 0.64870, acc: 0.80000\n",
      "[Train] Step: 9500, loss: 0.78291, acc: 0.65000\n",
      "[Train] Step: 10000, loss: 0.68536, acc: 0.75000\n",
      "[Test ] Step: 10000, acc: 0.70100\n",
      "[Train] Step: 10500, loss: 0.78305, acc: 0.60000\n",
      "[Train] Step: 11000, loss: 0.57509, acc: 0.80000\n",
      "[Train] Step: 11500, loss: 0.39825, acc: 0.90000\n",
      "[Train] Step: 12000, loss: 0.59367, acc: 0.75000\n",
      "[Train] Step: 12500, loss: 0.49273, acc: 0.85000\n",
      "[Train] Step: 13000, loss: 0.47508, acc: 0.85000\n",
      "[Train] Step: 13500, loss: 0.65638, acc: 0.70000\n",
      "[Train] Step: 14000, loss: 0.43118, acc: 0.90000\n",
      "[Train] Step: 14500, loss: 0.66836, acc: 0.75000\n",
      "[Train] Step: 15000, loss: 0.37303, acc: 0.85000\n",
      "[Test ] Step: 15000, acc: 0.78300\n",
      "[Train] Step: 15500, loss: 0.61306, acc: 0.75000\n",
      "[Train] Step: 16000, loss: 0.76190, acc: 0.70000\n",
      "[Train] Step: 16500, loss: 0.58446, acc: 0.95000\n",
      "[Train] Step: 17000, loss: 0.58673, acc: 0.80000\n",
      "[Train] Step: 17500, loss: 0.74767, acc: 0.80000\n",
      "[Train] Step: 18000, loss: 0.76638, acc: 0.70000\n",
      "[Train] Step: 18500, loss: 0.72757, acc: 0.80000\n",
      "[Train] Step: 19000, loss: 0.39964, acc: 0.80000\n",
      "[Train] Step: 19500, loss: 0.14460, acc: 1.00000\n",
      "[Train] Step: 20000, loss: 0.82671, acc: 0.70000\n",
      "[Test ] Step: 20000, acc: 0.78200\n",
      "[Train] Step: 20500, loss: 0.29388, acc: 0.90000\n",
      "[Train] Step: 21000, loss: 0.86208, acc: 0.55000\n",
      "[Train] Step: 21500, loss: 0.32807, acc: 0.90000\n",
      "[Train] Step: 22000, loss: 0.66067, acc: 0.80000\n",
      "[Train] Step: 22500, loss: 0.82406, acc: 0.80000\n",
      "[Train] Step: 23000, loss: 0.25542, acc: 0.95000\n",
      "[Train] Step: 23500, loss: 0.69497, acc: 0.75000\n",
      "[Train] Step: 24000, loss: 0.59936, acc: 0.85000\n",
      "[Train] Step: 24500, loss: 0.44653, acc: 0.85000\n",
      "[Train] Step: 25000, loss: 0.46182, acc: 0.95000\n",
      "[Test ] Step: 25000, acc: 0.76150\n",
      "[Train] Step: 25500, loss: 0.26655, acc: 0.90000\n",
      "[Train] Step: 26000, loss: 0.37704, acc: 0.85000\n",
      "[Train] Step: 26500, loss: 0.75437, acc: 0.65000\n",
      "[Train] Step: 27000, loss: 0.24038, acc: 0.90000\n",
      "[Train] Step: 27500, loss: 0.15164, acc: 0.95000\n",
      "[Train] Step: 28000, loss: 0.32839, acc: 0.85000\n",
      "[Train] Step: 28500, loss: 0.37404, acc: 0.90000\n",
      "[Train] Step: 29000, loss: 0.79323, acc: 0.80000\n",
      "[Train] Step: 29500, loss: 0.22697, acc: 0.90000\n",
      "[Train] Step: 30000, loss: 0.44106, acc: 0.85000\n",
      "[Test ] Step: 30000, acc: 0.80500\n",
      "[Train] Step: 30500, loss: 0.49448, acc: 0.80000\n",
      "[Train] Step: 31000, loss: 0.66619, acc: 0.85000\n",
      "[Train] Step: 31500, loss: 0.47225, acc: 0.85000\n",
      "[Train] Step: 32000, loss: 0.38756, acc: 0.85000\n",
      "[Train] Step: 32500, loss: 0.45793, acc: 0.80000\n",
      "[Train] Step: 33000, loss: 0.58669, acc: 0.75000\n",
      "[Train] Step: 33500, loss: 0.42713, acc: 0.85000\n",
      "[Train] Step: 34000, loss: 0.57525, acc: 0.75000\n",
      "[Train] Step: 34500, loss: 0.42680, acc: 0.80000\n",
      "[Train] Step: 35000, loss: 0.22665, acc: 0.95000\n",
      "[Test ] Step: 35000, acc: 0.80950\n",
      "[Train] Step: 35500, loss: 0.20880, acc: 1.00000\n",
      "[Train] Step: 36000, loss: 0.24418, acc: 0.95000\n",
      "[Train] Step: 36500, loss: 0.68701, acc: 0.70000\n",
      "[Train] Step: 37000, loss: 0.12868, acc: 0.95000\n",
      "[Train] Step: 37500, loss: 0.44746, acc: 0.80000\n",
      "[Train] Step: 38000, loss: 0.45837, acc: 0.85000\n",
      "[Train] Step: 38500, loss: 0.14352, acc: 0.95000\n",
      "[Train] Step: 39000, loss: 0.44649, acc: 0.80000\n",
      "[Train] Step: 39500, loss: 0.62614, acc: 0.75000\n",
      "[Train] Step: 40000, loss: 0.14579, acc: 0.95000\n",
      "[Test ] Step: 40000, acc: 0.79000\n",
      "[Train] Step: 40500, loss: 0.40009, acc: 0.80000\n",
      "[Train] Step: 41000, loss: 0.39855, acc: 0.80000\n",
      "[Train] Step: 41500, loss: 0.36009, acc: 0.80000\n",
      "[Train] Step: 42000, loss: 0.48817, acc: 0.70000\n",
      "[Train] Step: 42500, loss: 0.48341, acc: 0.85000\n",
      "[Train] Step: 43000, loss: 1.07185, acc: 0.70000\n",
      "[Train] Step: 43500, loss: 0.20448, acc: 0.90000\n",
      "[Train] Step: 44000, loss: 0.66958, acc: 0.75000\n",
      "[Train] Step: 44500, loss: 0.30022, acc: 0.90000\n",
      "[Train] Step: 45000, loss: 0.11946, acc: 0.95000\n",
      "[Test ] Step: 45000, acc: 0.79900\n",
      "[Train] Step: 45500, loss: 0.28832, acc: 0.90000\n",
      "[Train] Step: 46000, loss: 0.10540, acc: 1.00000\n",
      "[Train] Step: 46500, loss: 0.42017, acc: 0.90000\n",
      "[Train] Step: 47000, loss: 0.35612, acc: 0.85000\n",
      "[Train] Step: 47500, loss: 0.92081, acc: 0.70000\n",
      "[Train] Step: 48000, loss: 0.55982, acc: 0.80000\n",
      "[Train] Step: 48500, loss: 0.58921, acc: 0.85000\n",
      "[Train] Step: 49000, loss: 0.24139, acc: 0.90000\n",
      "[Train] Step: 49500, loss: 0.07866, acc: 1.00000\n",
      "[Train] Step: 50000, loss: 0.32136, acc: 0.85000\n",
      "[Test ] Step: 50000, acc: 0.81750\n",
      "[Train] Step: 50500, loss: 0.48827, acc: 0.75000\n",
      "[Train] Step: 51000, loss: 0.25786, acc: 0.95000\n",
      "[Train] Step: 51500, loss: 0.43270, acc: 0.90000\n",
      "[Train] Step: 52000, loss: 0.26486, acc: 0.90000\n",
      "[Train] Step: 52500, loss: 0.18226, acc: 0.95000\n",
      "[Train] Step: 53000, loss: 0.30173, acc: 0.90000\n",
      "[Train] Step: 53500, loss: 0.17365, acc: 0.90000\n",
      "[Train] Step: 54000, loss: 0.46077, acc: 0.85000\n",
      "[Train] Step: 54500, loss: 0.29430, acc: 0.85000\n",
      "[Train] Step: 55000, loss: 0.34833, acc: 0.85000\n",
      "[Test ] Step: 55000, acc: 0.80800\n",
      "[Train] Step: 55500, loss: 0.24107, acc: 0.90000\n",
      "[Train] Step: 56000, loss: 0.39169, acc: 0.80000\n",
      "[Train] Step: 56500, loss: 0.28122, acc: 0.85000\n",
      "[Train] Step: 57000, loss: 0.27545, acc: 0.90000\n",
      "[Train] Step: 57500, loss: 0.37249, acc: 0.80000\n",
      "[Train] Step: 58000, loss: 0.32224, acc: 0.90000\n",
      "[Train] Step: 58500, loss: 0.18292, acc: 0.95000\n",
      "[Train] Step: 59000, loss: 0.20634, acc: 0.90000\n",
      "[Train] Step: 59500, loss: 0.24001, acc: 0.90000\n",
      "[Train] Step: 60000, loss: 0.32481, acc: 0.85000\n",
      "[Test ] Step: 60000, acc: 0.81950\n",
      "[Train] Step: 60500, loss: 0.26861, acc: 0.90000\n",
      "[Train] Step: 61000, loss: 0.18993, acc: 0.95000\n",
      "[Train] Step: 61500, loss: 0.20819, acc: 0.95000\n",
      "[Train] Step: 62000, loss: 0.43176, acc: 0.80000\n",
      "[Train] Step: 62500, loss: 0.33691, acc: 0.80000\n",
      "[Train] Step: 63000, loss: 0.06783, acc: 1.00000\n",
      "[Train] Step: 63500, loss: 0.28188, acc: 0.90000\n",
      "[Train] Step: 64000, loss: 0.13565, acc: 0.95000\n",
      "[Train] Step: 64500, loss: 0.29331, acc: 0.85000\n",
      "[Train] Step: 65000, loss: 0.28250, acc: 0.85000\n",
      "[Test ] Step: 65000, acc: 0.82250\n",
      "[Train] Step: 65500, loss: 0.28669, acc: 0.90000\n",
      "[Train] Step: 66000, loss: 0.45186, acc: 0.75000\n",
      "[Train] Step: 66500, loss: 0.47435, acc: 0.85000\n",
      "[Train] Step: 67000, loss: 0.10794, acc: 0.95000\n",
      "[Train] Step: 67500, loss: 0.07361, acc: 1.00000\n",
      "[Train] Step: 68000, loss: 0.79016, acc: 0.70000\n",
      "[Train] Step: 68500, loss: 0.24023, acc: 0.90000\n",
      "[Train] Step: 69000, loss: 0.18609, acc: 0.95000\n",
      "[Train] Step: 69500, loss: 0.18719, acc: 1.00000\n",
      "[Train] Step: 70000, loss: 0.49397, acc: 0.80000\n",
      "[Test ] Step: 70000, acc: 0.81500\n",
      "[Train] Step: 70500, loss: 0.45615, acc: 0.85000\n",
      "[Train] Step: 71000, loss: 0.07510, acc: 1.00000\n",
      "[Train] Step: 71500, loss: 0.28199, acc: 0.90000\n",
      "[Train] Step: 72000, loss: 0.34450, acc: 0.85000\n",
      "[Train] Step: 72500, loss: 0.39514, acc: 0.90000\n",
      "[Train] Step: 73000, loss: 0.43953, acc: 0.90000\n",
      "[Train] Step: 73500, loss: 0.38059, acc: 0.80000\n",
      "[Train] Step: 74000, loss: 0.49036, acc: 0.80000\n",
      "[Train] Step: 74500, loss: 0.32963, acc: 0.85000\n",
      "[Train] Step: 75000, loss: 0.07353, acc: 1.00000\n",
      "[Test ] Step: 75000, acc: 0.81200\n",
      "[Train] Step: 75500, loss: 0.29013, acc: 0.85000\n",
      "[Train] Step: 76000, loss: 0.28031, acc: 0.90000\n",
      "[Train] Step: 76500, loss: 0.10542, acc: 1.00000\n",
      "[Train] Step: 77000, loss: 0.17731, acc: 0.95000\n",
      "[Train] Step: 77500, loss: 0.13722, acc: 0.95000\n",
      "[Train] Step: 78000, loss: 0.40844, acc: 0.85000\n",
      "[Train] Step: 78500, loss: 0.03470, acc: 1.00000\n",
      "[Train] Step: 79000, loss: 0.12706, acc: 0.95000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 79500, loss: 0.39341, acc: 0.90000\n",
      "[Train] Step: 80000, loss: 0.36186, acc: 0.80000\n",
      "[Test ] Step: 80000, acc: 0.80800\n",
      "[Train] Step: 80500, loss: 0.21988, acc: 0.90000\n",
      "[Train] Step: 81000, loss: 0.09472, acc: 1.00000\n",
      "[Train] Step: 81500, loss: 0.03393, acc: 1.00000\n",
      "[Train] Step: 82000, loss: 0.16701, acc: 0.90000\n",
      "[Train] Step: 82500, loss: 0.29554, acc: 0.90000\n",
      "[Train] Step: 83000, loss: 0.35270, acc: 0.80000\n",
      "[Train] Step: 83500, loss: 0.18461, acc: 0.95000\n",
      "[Train] Step: 84000, loss: 0.21857, acc: 0.85000\n",
      "[Train] Step: 84500, loss: 0.36335, acc: 0.90000\n",
      "[Train] Step: 85000, loss: 0.21146, acc: 0.95000\n",
      "[Test ] Step: 85000, acc: 0.81450\n",
      "[Train] Step: 85500, loss: 0.33240, acc: 0.85000\n",
      "[Train] Step: 86000, loss: 0.10989, acc: 0.95000\n",
      "[Train] Step: 86500, loss: 0.26680, acc: 0.90000\n",
      "[Train] Step: 87000, loss: 0.16079, acc: 0.90000\n",
      "[Train] Step: 87500, loss: 0.34909, acc: 0.90000\n",
      "[Train] Step: 88000, loss: 0.21068, acc: 0.90000\n",
      "[Train] Step: 88500, loss: 0.06257, acc: 1.00000\n",
      "[Train] Step: 89000, loss: 0.30066, acc: 0.85000\n",
      "[Train] Step: 89500, loss: 0.18051, acc: 0.95000\n",
      "[Train] Step: 90000, loss: 0.18148, acc: 0.95000\n",
      "[Test ] Step: 90000, acc: 0.82050\n",
      "[Train] Step: 90500, loss: 0.23125, acc: 0.85000\n",
      "[Train] Step: 91000, loss: 0.03314, acc: 1.00000\n",
      "[Train] Step: 91500, loss: 0.48016, acc: 0.90000\n",
      "[Train] Step: 92000, loss: 0.32668, acc: 0.90000\n",
      "[Train] Step: 92500, loss: 0.08497, acc: 1.00000\n",
      "[Train] Step: 93000, loss: 0.34242, acc: 0.85000\n",
      "[Train] Step: 93500, loss: 0.26557, acc: 0.85000\n",
      "[Train] Step: 94000, loss: 0.24418, acc: 0.90000\n",
      "[Train] Step: 94500, loss: 0.23258, acc: 0.90000\n",
      "[Train] Step: 95000, loss: 0.25491, acc: 0.85000\n",
      "[Test ] Step: 95000, acc: 0.82450\n",
      "[Train] Step: 95500, loss: 0.10875, acc: 0.95000\n",
      "[Train] Step: 96000, loss: 0.16548, acc: 0.95000\n",
      "[Train] Step: 96500, loss: 0.17341, acc: 0.85000\n",
      "[Train] Step: 97000, loss: 0.15154, acc: 0.95000\n",
      "[Train] Step: 97500, loss: 0.11772, acc: 1.00000\n",
      "[Train] Step: 98000, loss: 0.22184, acc: 0.95000\n",
      "[Train] Step: 98500, loss: 0.24444, acc: 0.90000\n",
      "[Train] Step: 99000, loss: 0.07345, acc: 0.95000\n",
      "[Train] Step: 99500, loss: 0.03047, acc: 1.00000\n",
      "[Train] Step: 100000, loss: 0.12475, acc: 0.95000\n",
      "[Test ] Step: 100000, acc: 0.79950\n",
      "[Train] Step: 100500, loss: 0.50647, acc: 0.90000\n",
      "[Train] Step: 101000, loss: 0.56364, acc: 0.90000\n",
      "[Train] Step: 101500, loss: 0.29240, acc: 0.90000\n",
      "[Train] Step: 102000, loss: 0.30970, acc: 0.90000\n",
      "[Train] Step: 102500, loss: 0.31203, acc: 0.90000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7019a39229a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;31m# 获得 损失值, 准确率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0meval_val_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# 在训练的时候，is_train 为 True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_val_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import  os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 文件存放目录\n",
    "CIFAR_DIR = \"./cifar-10-batches-py\"\n",
    "\n",
    "# tensorboard\n",
    "# 1. 指定面板图上显示的变量\n",
    "# 2. 训练过程中将这些变量计算出来,输出到文件中\n",
    "# 3. 文件解析 ./tensorboard  --logdir = dir.\n",
    "\n",
    "def load_data( filename ):\n",
    "    '''read data from data file'''\n",
    "    with open( filename, 'rb' ) as f:\n",
    "        data = pickle.load( f, encoding='bytes' ) # python3 需要添加上encoding='bytes'\n",
    "        return data[b'data'], data[b'labels'] # 并且 在 key 前需要加上 b\n",
    "\n",
    "class CifarData:\n",
    "    def __init__( self, filenames, need_shuffle ):\n",
    "        '''参数1:文件夹 参数2:是否需要随机打乱'''\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "\n",
    "        for filename in filenames:\n",
    "            # 将所有的数据,标签分别存放在两个list中\n",
    "            data, labels = load_data( filename )\n",
    "            all_data.append( data )\n",
    "            all_labels.append( labels )\n",
    "\n",
    "        # 将列表 组成 一个numpy类型的矩阵!!!!\n",
    "        self._data = np.vstack(all_data)\n",
    "        # 对数据进行归一化, 尺度固定在 [-1, 1] 之间\n",
    "        self._data = self._data\n",
    "        # 将列表,变成一个 numpy 数组\n",
    "        self._labels = np.hstack( all_labels )\n",
    "        # 记录当前的样本 数量\n",
    "        self._num_examples = self._data.shape[0]\n",
    "        # 保存是否需要随机打乱\n",
    "        self._need_shuffle = need_shuffle\n",
    "        # 样本的起始点\n",
    "        self._indicator = 0\n",
    "        # 判断是否需要打乱\n",
    "        if self._need_shuffle:\n",
    "            self._shffle_data()\n",
    "\n",
    "    def _shffle_data( self ):\n",
    "        # np.random.permutation() 从 0 到 参数,随机打乱\n",
    "        p = np.random.permutation( self._num_examples )\n",
    "        # 保存 已经打乱 顺序的数据\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "\n",
    "    def next_batch( self, batch_size ):\n",
    "        '''return batch_size example as a batch'''\n",
    "        # 开始点 + 数量 = 结束点\n",
    "        end_indictor = self._indicator + batch_size\n",
    "        # 如果结束点大于样本数量\n",
    "        if end_indictor > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                # 重新打乱\n",
    "                self._shffle_data()\n",
    "                # 开始点归零,从头再来\n",
    "                self._indicator = 0\n",
    "                # 重新指定 结束点. 和上面的那一句,说白了就是重新开始\n",
    "                end_indictor = batch_size # 其实就是 0 + batch_size, 把 0 省略了\n",
    "            else:\n",
    "                raise Exception( \"have no more examples\" )\n",
    "        # 再次查看是否 超出边界了\n",
    "        if end_indictor > self._num_examples:\n",
    "            raise Exception( \"batch size is larger than all example\" )\n",
    "\n",
    "        # 把 batch 区间 的data和label保存,并最后return\n",
    "        batch_data = self._data[self._indicator:end_indictor]\n",
    "        batch_labels = self._labels[self._indicator:end_indictor]\n",
    "        self._indicator = end_indictor\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "# 拿到所有文件名称\n",
    "train_filename = [os.path.join(CIFAR_DIR, 'data_batch_%d' % i) for i in range(1, 6)]\n",
    "# 拿到标签\n",
    "test_filename = [os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "# 拿到训练数据和测试数据\n",
    "train_data = CifarData( train_filename, True )\n",
    "test_data = CifarData( test_filename, False )\n",
    "\n",
    "batch_size = 20\n",
    "# 设计计算图\n",
    "# 形状 [None, 3072] 3072 是 样本的维数, None 代表位置的样本数量\n",
    "x = tf.placeholder( tf.float32, [batch_size, 3072] )\n",
    "# 形状 [None] y的数量和x的样本数是对应的\n",
    "y = tf.placeholder( tf.int64, [batch_size] )\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "\n",
    "x_image = tf.reshape( x, [-1, 3, 32, 32] )\n",
    "# 将最开始的向量式的图片,转为真实的图片类型\n",
    "x_image = tf.transpose( x_image, perm= [0, 2, 3, 1] )\n",
    "\n",
    "# 将x_image拆分,eg: [(1, 32, 32, 3), (1, 32, 32, 3), ... ]\n",
    "x_image_arr = tf.split(x_image, num_or_size_splits = batch_size, axis = 0)\n",
    "\n",
    "# 用于存储增强后的图片\n",
    "result_x_image_arr = []\n",
    "\n",
    "for x_single_image in x_image_arr:\n",
    "    # 将x_single_image改变形状,改为图片的格式. eg:[1, 32, 32, 3] -> [32, 32, 3]\n",
    "    x_single_image = tf.reshape(x_single_image, [32, 32, 3])\n",
    "    # 上下反转\n",
    "    data_aug_1 = tf.image.random_flip_left_right(x_single_image)\n",
    "    # 增加亮度\n",
    "    data_aug_2 = tf.image.random_brightness(data_aug_1, max_delta = 63)\n",
    "    # 增加对比度\n",
    "    data_aug_3 = tf.image.random_contrast(data_aug_2, lower = 0.2, upper = 1.8)\n",
    "\n",
    "    # 将单张图片重新改成 四维\n",
    "    x_single_image = tf.reshape(data_aug_3, [1, 32, 32, 3])\n",
    "\n",
    "    # 将单张图片存入列表\n",
    "    result_x_image_arr.append(x_single_image)\n",
    "\n",
    "# 将result_x_image_arr重新合并成数据集的样子\n",
    "result_x_images = tf.concat(result_x_image_arr, axis = 0)\n",
    "\n",
    "# 重新做归一化\n",
    "normal_result_x_images = result_x_images / 127.5 - 1\n",
    "\n",
    "\"\"\"\n",
    "def conv_wrapper(inputs,\n",
    "                 name,\n",
    "                 output_channel = 32,\n",
    "                 kernel_size = (3, 3),\n",
    "                 activation = tf.nn.relu,\n",
    "                 padding = 'same'):\n",
    "    '''\n",
    "    tf.layers.conv2d 的包裹函数\n",
    "    :param inputs:\n",
    "    :param name:\n",
    "    :param output_channel:\n",
    "    :param kernel_size:\n",
    "    :param activation:\n",
    "    :param padding:\n",
    "    :return:\n",
    "    '''\n",
    "    return tf.layers.conv2d(inputs,\n",
    "                            output_channel,\n",
    "                            kernel_size,\n",
    "                            padding = padding,\n",
    "                            activation = activation,\n",
    "                            name = name)\n",
    "\"\"\"\n",
    "\n",
    "def conv_wrapper(inputs,\n",
    "                 name,\n",
    "                 is_training,\n",
    "                 output_channel = 32,\n",
    "                 kernel_size = (3, 3),\n",
    "                 activation = tf.nn.relu,\n",
    "                 padding = 'same'):\n",
    "    '''\n",
    "    卷积层 包裹函数\n",
    "    :param inputs:\n",
    "    :param name:\n",
    "    :param is_training:\n",
    "    :param output_channel:\n",
    "    :param kernel_size:\n",
    "    :param activation:\n",
    "    :param padding:\n",
    "    :return:\n",
    "    '''\n",
    "    # without bn: conv -> activation\n",
    "    # with batch normalization: conv -> bn -> activation\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        conv2d = tf.layers.conv2d(inputs,\n",
    "                                output_channel,\n",
    "                                kernel_size,\n",
    "                                padding = padding,\n",
    "                                activation = None,\n",
    "                                name = name + '/conv2d')\n",
    "        # 第二个参数很重要，normalization需要维护一个均值和一个方差，\n",
    "        # 在训练过程和测试过程中，他们的值是不一样的，\n",
    "        # 在训练上，均值和方差是在一个batch上计算得到的\n",
    "        # 预测过程中，均值和方差是在整个数据集上，通过加权平均计算得到的\n",
    "        # 所以，在训练和测试中模式是不一样的，\n",
    "        # 在 train 中，设置为 True，在test中，设置为False\n",
    "        bn = tf.layers.batch_normalization(conv2d,\n",
    "                                           training = is_training)\n",
    "        return activation(bn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pooling_wrapper(inputs, name):\n",
    "    '''\n",
    "    tf.layers.max_pooling2d 的包裹函数\n",
    "    :param inputs:\n",
    "    :param name:\n",
    "    :return:\n",
    "    '''\n",
    "    return tf.layers.max_pooling2d(inputs,\n",
    "                                   (2, 2),\n",
    "                                   (2, 2),\n",
    "                                   name = name)\n",
    "\n",
    "\n",
    "# conv1:神经元 feature_map 输出图像  图像大小: 32 * 32\n",
    "conv1_1 = conv_wrapper(normal_result_x_images, 'conv1_1', is_training)\n",
    "\n",
    "conv1_2 = conv_wrapper(conv1_1, 'conv1_2', is_training)\n",
    "conv1_3 = conv_wrapper(conv1_2, 'conv1_3', is_training)\n",
    "# 池化层 图像输出为: 16 * 16\n",
    "pooling1 = pooling_wrapper(conv1_3, 'pooling1')\n",
    "conv2_1 = conv_wrapper(pooling1, 'conv2_1', is_training)\n",
    "conv2_2 = conv_wrapper(conv2_1, 'conv2_2', is_training)\n",
    "conv2_3 = conv_wrapper(conv2_2, 'conv2_4', is_training)\n",
    "# 池化层 图像输出为 8 * 8\n",
    "pooling2 = pooling_wrapper(conv2_3, 'pooling2')\n",
    "conv3_1 = conv_wrapper(pooling2, 'conv3_1', is_training)\n",
    "conv3_2 = conv_wrapper(conv3_1, 'conv3_2', is_training)\n",
    "conv3_3 = conv_wrapper(conv3_2, 'conv3_3', is_training)\n",
    "# 池化层 输出为 4 * 4 * 32\n",
    "pooling3 = pooling_wrapper(conv3_3, 'pooling3')\n",
    "# 展平\n",
    "flatten  = tf.contrib.layers.flatten( pooling3 )\n",
    "y_ = tf.layers.dense(flatten, 10)\n",
    "\n",
    "\n",
    "# 使用交叉熵 设置损失函数\n",
    "loss = tf.losses.sparse_softmax_cross_entropy( labels = y, logits = y_ )\n",
    "# 该api,做了三件事儿 1. y_ -> softmax 2. y -> one_hot 3. loss = ylogy\n",
    "\n",
    "# 预测值 获得的是 每一行上 最大值的 索引.注意:tf.argmax()的用法,其实和 np.argmax() 一样的\n",
    "predict = tf.argmax( y_, 1 )\n",
    "# 将布尔值转化为int类型,也就是 0 或者 1, 然后再和真实值进行比较. tf.equal() 返回值是布尔类型\n",
    "correct_prediction = tf.equal( predict, y )\n",
    "# 比如说第一行最大值索引是6,说明是第六个分类.而y正好也是6,说明预测正确\n",
    "\n",
    "\n",
    "\n",
    "# 将上句的布尔类型 转化为 浮点类型,然后进行求平均值,实际上就是求出了准确率\n",
    "accuracy = tf.reduce_mean( tf.cast(correct_prediction, tf.float64) )\n",
    "\n",
    "with tf.name_scope( 'train_op' ): \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = tf.train.AdamOptimizer( 1e-3 ).minimize( loss ) # 将 损失函数 降到 最低\n",
    "\n",
    "\n",
    "def variable_summary(var, name):\n",
    "    '''\n",
    "    一个变量的各种统计量,建立一个summary\n",
    "    :param var: 计算summary的变量\n",
    "    :param name: 指定命名空间,以防冲突\n",
    "    :return:\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        with tf.name_scope('stddev'):\n",
    "            # 求标准差\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('mean', mean) # 均值\n",
    "            tf.summary.scalar('stddev', stddev) # 标准差\n",
    "            tf.summary.scalar('min', tf.reduce_min(var)) # 最小值\n",
    "            tf.summary.scalar('max', tf.reduce_max(var)) # 最大值\n",
    "            tf.summary.histogram('histogram', var) # 直方图 反应的是变量的分布\n",
    "\n",
    "\n",
    "# 给六个卷积层添加summary\n",
    "with tf.name_scope('summary'):\n",
    "    variable_summary(conv1_1, 'conv1_1')\n",
    "    variable_summary(conv1_2, 'conv1_2')\n",
    "    variable_summary(conv2_1, 'conv2_1')\n",
    "    variable_summary(conv2_2, 'conv2_2')\n",
    "    variable_summary(conv3_1, 'conv3_1')\n",
    "    variable_summary(conv3_2, 'conv3_2')\n",
    "\n",
    "\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "# 'loss':<10, 1.1>, <20, 1.08>\n",
    "accuracy_summary = tf.summary.scalar('accurary', accuracy)\n",
    "\n",
    "inputs_summary = tf.summary.image('inputs_image', normal_result_x_images)\n",
    "\n",
    "merged_summary = tf.summary.merge_all() # 将以上所有带有 summary 的变量聚合起来\n",
    "merged_summary_test = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "\n",
    "# 指定文件保存路径\n",
    "LOG_DIR = '.'\n",
    "run_label = 'run_vgg_tensorboard'\n",
    "run_dir = os.path.join(LOG_DIR, run_label)\n",
    "# 判断该文件夹是否已经创建\n",
    "if not os.path.exists(run_dir):\n",
    "    os.mkdir(run_dir)\n",
    "# 在该文件夹下创建两个文件夹,一个存放训练数据,一个存放测试数据\n",
    "train_log_dir = os.path.join(run_dir, 'train')\n",
    "test_log_dir = os.path.join(run_dir, 'test')\n",
    "# 判断这两个文件夹是否存在\n",
    "if not os.path.exists(train_log_dir):\n",
    "    os.mkdir(train_log_dir)\n",
    "if not os.path.exists(test_log_dir):\n",
    "    os.mkdir(test_log_dir)\n",
    "\n",
    "\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_steps = 100000\n",
    "test_steps = 100\n",
    "\n",
    "#　不是每一步summary都是要计算的可以定义一个范围,每过多少步计算一次\n",
    "output_summary_every_steps = 100\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run( init ) # 注意: 这一步必须要有!!\n",
    "\n",
    "    # 打开一个writer,向writer中写数据\n",
    "    train_writer = tf.summary.FileWriter(train_log_dir, sess.graph) # 参数2:显示计算图\n",
    "    test_writer = tf.summary.FileWriter(test_log_dir)\n",
    "\n",
    "    fixed_test_batch_data, fixed_test_batch_labels = test_data.next_batch(batch_size)\n",
    "\n",
    "    # 开始训练\n",
    "    for i in range( train_steps ):\n",
    "        # 得到batch\n",
    "        batch_data, batch_labels = train_data.next_batch( batch_size )\n",
    "\n",
    "\n",
    "        eval_ops = [loss, accuracy, train_op]\n",
    "        should_output_summary = ((i+1) % output_summary_every_steps == 0)\n",
    "\n",
    "        if should_output_summary:\n",
    "            eval_ops.append(merged_summary)\n",
    "\n",
    "\n",
    "        # 获得 损失值, 准确率\n",
    "        eval_val_results = sess.run( eval_ops, feed_dict={x:batch_data, y:batch_labels, is_training:True} ) # 在训练的时候，is_train 为 True\n",
    "        loss_val, acc_val = eval_val_results[0:2]\n",
    "\n",
    "\n",
    "\n",
    "        if should_output_summary:\n",
    "            train_summary_str = eval_val_results[-1]\n",
    "            train_writer.add_summary(train_summary_str, i+1)\n",
    "            # 在 测试 时候，is_trian 为 False\n",
    "            test_summary_str = sess.run([merged_summary_test],\n",
    "                                        feed_dict = {x: fixed_test_batch_data,y: fixed_test_batch_labels, is_training: False} )[0]\n",
    "            test_writer.add_summary(test_summary_str, i+1)\n",
    "\n",
    "\n",
    "\n",
    "        # 每 500 次 输出一条信息\n",
    "        if ( i+1 ) % 500 == 0:\n",
    "            print('[Train] Step: %d, loss: %4.5f, acc: %4.5f' % ( i+1, loss_val, acc_val ))\n",
    "        # 每 5000 次 进行一次 测试\n",
    "        if ( i+1 ) % 5000 == 0:\n",
    "            # 获取数据集,但不随机\n",
    "            test_data = CifarData( test_filename, False )\n",
    "            all_test_acc_val = []\n",
    "            for j in range( test_steps ):\n",
    "                test_batch_data, test_batch_labels = test_data.next_batch( batch_size )\n",
    "                test_acc_val = sess.run( [accuracy], feed_dict={ x:test_batch_data, y:test_batch_labels, is_training:False } )\n",
    "                all_test_acc_val.append( test_acc_val )\n",
    "            test_acc = np.mean( all_test_acc_val )\n",
    "\n",
    "            print('[Test ] Step: %d, acc: %4.5f' % ( (i+1), test_acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
