{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batches.meta', 'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'readme.html', 'test_batch']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "CIFAR_DIR = './cifar-10-batches-py'\n",
    "print(os.listdir(CIFAR_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "# 1. 指定面板图上显示的变量\n",
    "# 2. 训练过程中将这些变量计算出来，输出到文件中\n",
    "# 3. 文件解析 ./tensorboard --logdir=dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (50000,)\n",
      "(10000, 3072) (10000,)\n",
      "[[196 200 204 ... 222 223 225]\n",
      " [144 138 137 ... 169 160 137]\n",
      " [131 203 237 ...  46  48  46]\n",
      " ...\n",
      " [197 224 224 ...  86 117 127]\n",
      " [118 106 128 ... 191 214 216]\n",
      " [ 88  83  78 ... 224 222 218]] [5 6 7 3 1 8 2 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"read data from data file.\"\"\"\n",
    "    with open (filename, \"rb\") as f:\n",
    "        data = pickle.load(f,encoding='bytes')\n",
    "        return data[b'data'], data[b'labels']\n",
    "    \n",
    "class CifarData:\n",
    "    def __init__(self, filenames, need_shuffle):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        for filename in filenames:\n",
    "            data, labels = load_data(filename)\n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)\n",
    "        self._data = np.vstack(all_data)\n",
    "        self._data = self._data\n",
    "        self._labels = np.hstack(all_labels)\n",
    "        print(self._data.shape, self._labels.shape)\n",
    "        self._num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0 ## 遍历起始位置\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "    \n",
    "    def _shuffle_data(self):\n",
    "        # [0, 1, 2, 3, 4, 5] -> [5, 3, 2, 4, 0 ,1]\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0 \n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more example\")\n",
    "        \n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator: end_indicator]    \n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "train_filenames = [os.path.join(CIFAR_DIR, 'data_batch_%d' % i) for i in range(1,6)]\n",
    "test_filenames = [os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "train_data = CifarData(train_filenames, True)\n",
    "test_data = CifarData(test_filenames, False)\n",
    "\n",
    "batch_data, batch_labels = train_data.next_batch(10)\n",
    "print(batch_data, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 17:33:37.579937 16756 deprecation.py:323] From <ipython-input-4-e9fb6d5f95e1>:39: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0718 17:33:37.591918 16756 deprecation.py:506] From D:\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0718 17:33:38.407915 16756 deprecation.py:323] From <ipython-input-4-e9fb6d5f95e1>:40: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W0718 17:33:39.653172 16756 deprecation.py:323] From <ipython-input-4-e9fb6d5f95e1>:48: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0718 17:33:41.436476 16756 deprecation.py:323] From <ipython-input-4-e9fb6d5f95e1>:68: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0718 17:33:42.069742 16756 deprecation.py:323] From <ipython-input-4-e9fb6d5f95e1>:69: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0718 17:33:43.107091 16756 deprecation.py:323] From D:\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size, 3072])\n",
    "\n",
    "# [None], eg: [0,6,5,3]\n",
    "y = tf.placeholder(tf.int64, [batch_size])\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 3, 32, 32])\n",
    "# 32*32\n",
    "x_image = tf.transpose(x_image, perm=[0, 2, 3, 1])\n",
    "\n",
    "x_image_arr = tf.split(x_image, num_or_size_splits=batch_size, axis=0)\n",
    "result_x_image_arr = []\n",
    "\n",
    "for x_single_image in x_image_arr:\n",
    "    # x_single_image: [1, 32, 32, 3] -> [32, 32, 3]\n",
    "    x_single_image = tf.reshape(x_single_image, [32, 32, 3])\n",
    "    data_aug_1 = tf.image.random_flip_left_right(x_single_image)\n",
    "    data_aug_2 = tf.image.random_brightness(data_aug_1, max_delta=65)\n",
    "    data_aug_3 = tf.image.random_contrast(data_aug_2, lower=0.2, upper=1.8)\n",
    "    x_single_image = tf.reshape(data_aug_3, [1, 32, 32, 3])\n",
    "    result_x_image_arr.append(x_single_image)\n",
    "result_x_images = tf.concat(result_x_image_arr, axis=0)\n",
    "normal_result_x_images = result_x_images / 127.5 - 1\n",
    "    \n",
    "    \n",
    "def conv_wrapper(inputs, name, is_training, output_channel=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu):\n",
    "    \"\"\"wrapper of tf.layers.conv2d\"\"\"\n",
    "    # without bn: conv -> activation\n",
    "    # with batch normalization: conv -> bn -> activation\n",
    "    with tf.name_scope(name):\n",
    "        conv2d = tf.layers.conv2d(inputs,\n",
    "                               output_channel,\n",
    "                               kernel_size,\n",
    "                               padding=padding,\n",
    "                               activation=None,\n",
    "                               name=name + 'conv12d')\n",
    "        bn = tf.layers.batch_normalization(conv2d, training = is_training)\n",
    "        return activation(bn)\n",
    "    \n",
    "def pooling_wrapper(inputs, name):\n",
    "    \"\"\"wrapper of tf.layers.conv2d\"\"\"\n",
    "    return tf.layers.max_pooling2d(inputs,\n",
    "                                  (2,2),\n",
    "                                  (2,2),\n",
    "                                  name=name)\n",
    "\n",
    "conv1_1 = conv_wrapper(normal_result_x_images, 'conv1_1', is_training)\n",
    "conv1_2 = conv_wrapper(conv1_1, 'conv1_2', is_training)\n",
    "conv1_3 = conv_wrapper(conv1_2, 'conv1_3', is_training)\n",
    "pooling1 = pooling_wrapper(conv1_3, 'pool1')\n",
    "\n",
    "conv2_1 = conv_wrapper(pooling1, 'conv2_1', is_training)\n",
    "conv2_2 = conv_wrapper(conv2_1, 'conv2_2', is_training)\n",
    "conv2_3 = conv_wrapper(conv2_2, 'conv2_3', is_training)\n",
    "pooling2 = pooling_wrapper(conv2_3, 'pool2')\n",
    "\n",
    "conv3_1 = conv_wrapper(pooling2, 'conv3_1', is_training)\n",
    "conv3_2 = conv_wrapper(conv3_1, 'conv3_2', is_training)\n",
    "conv3_3 = conv_wrapper(conv3_2, 'conv3_3', is_training)\n",
    "pooling3 = pooling_wrapper(conv3_3, 'pool3')\n",
    "\n",
    "# [None, 4 * 4 * 32]\n",
    "flatten = tf.layers.flatten(pooling3)\n",
    "y_ = tf.layers.dense(flatten, 10)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y,logits=y_)\n",
    "# y_ -> softmax\n",
    "# y -> one_hot\n",
    "#loss = ylog_\n",
    "\n",
    "# indices\n",
    "predict = tf.argmax(y_, 1)\n",
    "# [1,0,1,0]\n",
    "correct_prediction = tf.equal(predict, y)\n",
    "accuracy= tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summary(var, name):\n",
    "    \"\"\"Constructs summary fro statistics of a variable\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "    variable_summary(conv1_1, 'conv1_1')\n",
    "    variable_summary(conv1_2, 'conv1_2')\n",
    "    variable_summary(conv2_1, 'conv2_1')\n",
    "    variable_summary(conv2_2, 'conv2_2')\n",
    "    variable_summary(conv3_1, 'conv3_1')\n",
    "    variable_summary(conv3_2, 'conv3_2')\n",
    "        \n",
    "# 1. 指定面板图上显示的变量\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "# 'loss': <10, 1.1>, <20, 1.08>\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "inputs_summary = tf.summary.image('inputs_image', result_x_images)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "merged_summary_test = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "\n",
    "# 2. 训练过程中将这些变量计算出来，输出到文件中\n",
    "LOG_DIR = '.'\n",
    "run_label = 'run_vgg_tensorboard'\n",
    "run_dir = os.path.join(LOG_DIR, run_label)\n",
    "if not os.path.exists(run_dir):\n",
    "    os.mkdir(run_dir)\n",
    "train_log_dir = os.path.join(run_dir, 'train')\n",
    "test_log_dir = os.path.join(run_dir, 'test')\n",
    "\n",
    "if not os.path.exists(train_log_dir):\n",
    "    os.mkdir(train_log_dir)\n",
    "if not os.path.exists(test_log_dir):\n",
    "    os.mkdir(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] step: 0, loss: 2.79400, acc:0.20000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 1, acc: 0.08950\n",
      "[Train] step: 100, loss: 1.91471, acc:0.30000\n",
      "[Train] step: 200, loss: 2.34174, acc:0.30000\n",
      "[Train] step: 300, loss: 1.65327, acc:0.35000\n",
      "[Train] step: 400, loss: 2.00087, acc:0.25000\n",
      "[Train] step: 500, loss: 1.85610, acc:0.30000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 501, acc: 0.14400\n",
      "[Train] step: 600, loss: 1.63890, acc:0.35000\n",
      "[Train] step: 700, loss: 1.58881, acc:0.25000\n",
      "[Train] step: 800, loss: 1.89728, acc:0.30000\n",
      "[Train] step: 900, loss: 1.76466, acc:0.40000\n",
      "[Train] step: 1000, loss: 1.17290, acc:0.60000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 1001, acc: 0.15250\n",
      "[Train] step: 1100, loss: 0.98209, acc:0.70000\n",
      "[Train] step: 1200, loss: 1.94285, acc:0.25000\n",
      "[Train] step: 1300, loss: 1.17774, acc:0.60000\n",
      "[Train] step: 1400, loss: 1.09331, acc:0.75000\n",
      "[Train] step: 1500, loss: 1.48338, acc:0.30000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 1501, acc: 0.26100\n",
      "[Train] step: 1600, loss: 0.92773, acc:0.60000\n",
      "[Train] step: 1700, loss: 1.53820, acc:0.40000\n",
      "[Train] step: 1800, loss: 1.09459, acc:0.55000\n",
      "[Train] step: 1900, loss: 0.85466, acc:0.70000\n",
      "[Train] step: 2000, loss: 1.23914, acc:0.60000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 2001, acc: 0.22600\n",
      "[Train] step: 2100, loss: 0.93075, acc:0.80000\n",
      "[Train] step: 2200, loss: 1.38014, acc:0.50000\n",
      "[Train] step: 2300, loss: 0.86090, acc:0.80000\n",
      "[Train] step: 2400, loss: 1.15693, acc:0.60000\n",
      "[Train] step: 2500, loss: 0.85107, acc:0.70000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 2501, acc: 0.15900\n",
      "[Train] step: 2600, loss: 1.16955, acc:0.60000\n",
      "[Train] step: 2700, loss: 1.55675, acc:0.25000\n",
      "[Train] step: 2800, loss: 0.88920, acc:0.65000\n",
      "[Train] step: 2900, loss: 1.24181, acc:0.50000\n",
      "[Train] step: 3000, loss: 0.81758, acc:0.75000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 3001, acc: 0.21350\n",
      "[Train] step: 3100, loss: 1.29747, acc:0.50000\n",
      "[Train] step: 3200, loss: 0.78183, acc:0.75000\n",
      "[Train] step: 3300, loss: 1.29597, acc:0.45000\n",
      "[Train] step: 3400, loss: 0.78420, acc:0.70000\n",
      "[Train] step: 3500, loss: 0.60097, acc:0.70000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 3501, acc: 0.20700\n",
      "[Train] step: 3600, loss: 1.33028, acc:0.60000\n",
      "[Train] step: 3700, loss: 1.30931, acc:0.60000\n",
      "[Train] step: 3800, loss: 0.97346, acc:0.55000\n",
      "[Train] step: 3900, loss: 1.15436, acc:0.55000\n",
      "[Train] step: 4000, loss: 0.60068, acc:0.70000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 4001, acc: 0.20700\n",
      "[Train] step: 4100, loss: 1.46289, acc:0.45000\n",
      "[Train] step: 4200, loss: 0.99311, acc:0.60000\n",
      "[Train] step: 4300, loss: 0.81412, acc:0.80000\n",
      "[Train] step: 4400, loss: 0.73527, acc:0.65000\n",
      "[Train] step: 4500, loss: 0.73505, acc:0.75000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 4501, acc: 0.22450\n",
      "[Train] step: 4600, loss: 0.79029, acc:0.70000\n",
      "[Train] step: 4700, loss: 0.57402, acc:0.85000\n",
      "[Train] step: 4800, loss: 1.02358, acc:0.60000\n",
      "[Train] step: 4900, loss: 0.68901, acc:0.75000\n",
      "[Train] step: 5000, loss: 0.84534, acc:0.70000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 5001, acc: 0.23500\n",
      "[Train] step: 5100, loss: 1.09501, acc:0.60000\n",
      "[Train] step: 5200, loss: 0.40257, acc:0.85000\n",
      "[Train] step: 5300, loss: 1.10735, acc:0.55000\n",
      "[Train] step: 5400, loss: 0.66329, acc:0.80000\n",
      "[Train] step: 5500, loss: 0.92817, acc:0.65000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 5501, acc: 0.17250\n",
      "[Train] step: 5600, loss: 1.17852, acc:0.60000\n",
      "[Train] step: 5700, loss: 0.71293, acc:0.75000\n",
      "[Train] step: 5800, loss: 0.91981, acc:0.60000\n",
      "[Train] step: 5900, loss: 0.76524, acc:0.65000\n",
      "[Train] step: 6000, loss: 0.31866, acc:0.95000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 6001, acc: 0.21450\n",
      "[Train] step: 6100, loss: 0.84016, acc:0.80000\n",
      "[Train] step: 6200, loss: 0.93058, acc:0.65000\n",
      "[Train] step: 6300, loss: 0.54596, acc:0.90000\n",
      "[Train] step: 6400, loss: 0.90136, acc:0.55000\n",
      "[Train] step: 6500, loss: 0.94276, acc:0.65000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 6501, acc: 0.16500\n",
      "[Train] step: 6600, loss: 0.72214, acc:0.70000\n",
      "[Train] step: 6700, loss: 0.99591, acc:0.70000\n",
      "[Train] step: 6800, loss: 0.65035, acc:0.85000\n",
      "[Train] step: 6900, loss: 0.84731, acc:0.60000\n",
      "[Train] step: 7000, loss: 0.89987, acc:0.70000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 7001, acc: 0.19500\n",
      "[Train] step: 7100, loss: 0.69630, acc:0.75000\n",
      "[Train] step: 7200, loss: 1.15132, acc:0.60000\n",
      "[Train] step: 7300, loss: 0.87619, acc:0.75000\n",
      "[Train] step: 7400, loss: 0.89338, acc:0.65000\n",
      "[Train] step: 7500, loss: 0.80244, acc:0.75000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 7501, acc: 0.15400\n",
      "[Train] step: 7600, loss: 0.72780, acc:0.75000\n",
      "[Train] step: 7700, loss: 1.13142, acc:0.65000\n",
      "[Train] step: 7800, loss: 0.49232, acc:0.85000\n",
      "[Train] step: 7900, loss: 0.87482, acc:0.65000\n",
      "[Train] step: 8000, loss: 0.88578, acc:0.75000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 8001, acc: 0.18250\n",
      "[Train] step: 8100, loss: 0.32883, acc:0.95000\n",
      "[Train] step: 8200, loss: 0.55786, acc:0.80000\n",
      "[Train] step: 8300, loss: 0.85397, acc:0.75000\n",
      "[Train] step: 8400, loss: 0.53485, acc:0.80000\n",
      "[Train] step: 8500, loss: 0.35319, acc:0.95000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 8501, acc: 0.13800\n",
      "[Train] step: 8600, loss: 0.66168, acc:0.70000\n",
      "[Train] step: 8700, loss: 0.80232, acc:0.75000\n",
      "[Train] step: 8800, loss: 0.52798, acc:0.80000\n",
      "[Train] step: 8900, loss: 0.39710, acc:0.85000\n",
      "[Train] step: 9000, loss: 1.29713, acc:0.60000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 9001, acc: 0.14400\n",
      "[Train] step: 9100, loss: 0.50102, acc:0.85000\n",
      "[Train] step: 9200, loss: 0.55000, acc:0.90000\n",
      "[Train] step: 9300, loss: 0.86690, acc:0.80000\n",
      "[Train] step: 9400, loss: 0.79739, acc:0.75000\n",
      "[Train] step: 9500, loss: 0.50938, acc:0.80000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 9501, acc: 0.24150\n",
      "[Train] step: 9600, loss: 0.86117, acc:0.80000\n",
      "[Train] step: 9700, loss: 0.65787, acc:0.80000\n",
      "[Train] step: 9800, loss: 0.38267, acc:0.90000\n",
      "[Train] step: 9900, loss: 0.47027, acc:0.90000\n",
      "[Train] step: 10000, loss: 0.69657, acc:0.65000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 10001, acc: 0.18850\n",
      "[Train] step: 10100, loss: 0.82529, acc:0.70000\n",
      "[Train] step: 10200, loss: 0.81739, acc:0.65000\n",
      "[Train] step: 10300, loss: 0.56304, acc:0.80000\n",
      "[Train] step: 10400, loss: 0.99557, acc:0.75000\n",
      "[Train] step: 10500, loss: 0.76427, acc:0.75000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 10501, acc: 0.13450\n",
      "[Train] step: 10600, loss: 0.76334, acc:0.85000\n",
      "[Train] step: 10700, loss: 1.26533, acc:0.60000\n",
      "[Train] step: 10800, loss: 0.31988, acc:0.95000\n",
      "[Train] step: 10900, loss: 1.01874, acc:0.65000\n",
      "[Train] step: 11000, loss: 0.61508, acc:0.75000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 11001, acc: 0.17850\n",
      "[Train] step: 11100, loss: 0.73791, acc:0.65000\n",
      "[Train] step: 11200, loss: 0.38297, acc:0.85000\n",
      "[Train] step: 11300, loss: 0.69410, acc:0.85000\n",
      "[Train] step: 11400, loss: 0.47578, acc:0.80000\n",
      "[Train] step: 11500, loss: 0.68945, acc:0.70000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 11501, acc: 0.18150\n",
      "[Train] step: 11600, loss: 0.83673, acc:0.70000\n",
      "[Train] step: 11700, loss: 0.55446, acc:0.80000\n",
      "[Train] step: 11800, loss: 0.52367, acc:0.75000\n",
      "[Train] step: 11900, loss: 0.42107, acc:0.80000\n",
      "[Train] step: 12000, loss: 0.77525, acc:0.75000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 12001, acc: 0.19950\n",
      "[Train] step: 12100, loss: 0.53724, acc:0.80000\n",
      "[Train] step: 12200, loss: 0.54230, acc:0.75000\n",
      "[Train] step: 12300, loss: 1.00777, acc:0.65000\n",
      "[Train] step: 12400, loss: 0.46989, acc:0.80000\n",
      "[Train] step: 12500, loss: 0.80537, acc:0.65000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 12501, acc: 0.18700\n",
      "[Train] step: 12600, loss: 0.53961, acc:0.80000\n",
      "[Train] step: 12700, loss: 0.35501, acc:0.95000\n",
      "[Train] step: 12800, loss: 0.38272, acc:0.85000\n",
      "[Train] step: 12900, loss: 0.52963, acc:0.90000\n",
      "[Train] step: 13000, loss: 0.30071, acc:0.85000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 13001, acc: 0.17300\n",
      "[Train] step: 13100, loss: 0.54954, acc:0.70000\n",
      "[Train] step: 13200, loss: 0.50967, acc:0.80000\n",
      "[Train] step: 13300, loss: 0.87567, acc:0.70000\n",
      "[Train] step: 13400, loss: 0.27262, acc:0.95000\n",
      "[Train] step: 13500, loss: 0.35328, acc:0.90000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 13501, acc: 0.19650\n",
      "[Train] step: 13600, loss: 0.93878, acc:0.65000\n",
      "[Train] step: 13700, loss: 0.56344, acc:0.75000\n",
      "[Train] step: 13800, loss: 0.63501, acc:0.80000\n",
      "[Train] step: 13900, loss: 0.79482, acc:0.70000\n",
      "[Train] step: 14000, loss: 0.73882, acc:0.75000\n",
      "(10000, 3072) (10000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Step: 14001, acc: 0.18900\n",
      "[Train] step: 14100, loss: 1.20340, acc:0.60000\n",
      "[Train] step: 14200, loss: 0.42205, acc:0.90000\n",
      "[Train] step: 14300, loss: 0.58363, acc:0.80000\n",
      "[Train] step: 14400, loss: 0.37151, acc:0.80000\n",
      "[Train] step: 14500, loss: 0.30645, acc:0.90000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 14501, acc: 0.21250\n",
      "[Train] step: 14600, loss: 1.14112, acc:0.65000\n",
      "[Train] step: 14700, loss: 0.76709, acc:0.85000\n",
      "[Train] step: 14800, loss: 0.91343, acc:0.75000\n",
      "[Train] step: 14900, loss: 0.91947, acc:0.65000\n",
      "[Train] step: 15000, loss: 0.41005, acc:0.90000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 15001, acc: 0.18750\n",
      "[Train] step: 15100, loss: 0.73980, acc:0.75000\n",
      "[Train] step: 15200, loss: 0.40375, acc:0.90000\n",
      "[Train] step: 15300, loss: 1.05833, acc:0.70000\n",
      "[Train] step: 15400, loss: 0.45691, acc:0.85000\n",
      "[Train] step: 15500, loss: 0.27445, acc:0.85000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 15501, acc: 0.17200\n",
      "[Train] step: 15600, loss: 0.48053, acc:0.75000\n",
      "[Train] step: 15700, loss: 0.91942, acc:0.75000\n",
      "[Train] step: 15800, loss: 0.35672, acc:0.90000\n",
      "[Train] step: 15900, loss: 0.26584, acc:0.90000\n",
      "[Train] step: 16000, loss: 0.55083, acc:0.80000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 16001, acc: 0.18400\n",
      "[Train] step: 16100, loss: 0.67269, acc:0.75000\n",
      "[Train] step: 16200, loss: 0.39285, acc:0.85000\n",
      "[Train] step: 16300, loss: 0.35187, acc:0.80000\n",
      "[Train] step: 16400, loss: 1.22716, acc:0.70000\n",
      "[Train] step: 16500, loss: 0.55854, acc:0.85000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 16501, acc: 0.15900\n",
      "[Train] step: 16600, loss: 0.59554, acc:0.75000\n",
      "[Train] step: 16700, loss: 0.53032, acc:0.80000\n",
      "[Train] step: 16800, loss: 0.48704, acc:0.80000\n",
      "[Train] step: 16900, loss: 0.77159, acc:0.85000\n",
      "[Train] step: 17000, loss: 0.36775, acc:0.95000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 17001, acc: 0.18250\n",
      "[Train] step: 17100, loss: 0.50813, acc:0.80000\n",
      "[Train] step: 17200, loss: 0.45090, acc:0.85000\n",
      "[Train] step: 17300, loss: 0.32846, acc:0.85000\n",
      "[Train] step: 17400, loss: 0.50225, acc:0.80000\n",
      "[Train] step: 17500, loss: 0.70693, acc:0.75000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 17501, acc: 0.17850\n",
      "[Train] step: 17600, loss: 0.79180, acc:0.80000\n",
      "[Train] step: 17700, loss: 0.61891, acc:0.75000\n",
      "[Train] step: 17800, loss: 0.40877, acc:0.90000\n",
      "[Train] step: 17900, loss: 0.91633, acc:0.60000\n",
      "[Train] step: 18000, loss: 0.28238, acc:0.95000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 18001, acc: 0.15850\n",
      "[Train] step: 18100, loss: 0.87319, acc:0.75000\n",
      "[Train] step: 18200, loss: 0.23322, acc:0.90000\n",
      "[Train] step: 18300, loss: 0.68858, acc:0.80000\n",
      "[Train] step: 18400, loss: 0.30995, acc:0.90000\n",
      "[Train] step: 18500, loss: 0.29492, acc:0.90000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 18501, acc: 0.23450\n",
      "[Train] step: 18600, loss: 0.52181, acc:0.80000\n",
      "[Train] step: 18700, loss: 0.58273, acc:0.85000\n",
      "[Train] step: 18800, loss: 0.54698, acc:0.80000\n",
      "[Train] step: 18900, loss: 0.46697, acc:0.85000\n",
      "[Train] step: 19000, loss: 0.49741, acc:0.85000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 19001, acc: 0.20150\n",
      "[Train] step: 19100, loss: 0.60837, acc:0.80000\n",
      "[Train] step: 19200, loss: 0.49231, acc:0.85000\n",
      "[Train] step: 19300, loss: 0.41339, acc:0.85000\n",
      "[Train] step: 19400, loss: 0.36262, acc:0.85000\n",
      "[Train] step: 19500, loss: 1.25241, acc:0.60000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 19501, acc: 0.18200\n",
      "[Train] step: 19600, loss: 0.91681, acc:0.75000\n",
      "[Train] step: 19700, loss: 0.53982, acc:0.80000\n",
      "[Train] step: 19800, loss: 0.36723, acc:0.90000\n",
      "[Train] step: 19900, loss: 0.84593, acc:0.65000\n",
      "[Train] step: 20000, loss: 0.60253, acc:0.80000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 20001, acc: 0.18450\n",
      "[Train] step: 20100, loss: 0.42162, acc:0.90000\n",
      "[Train] step: 20200, loss: 0.70060, acc:0.70000\n",
      "[Train] step: 20300, loss: 0.82451, acc:0.65000\n",
      "[Train] step: 20400, loss: 0.34301, acc:0.90000\n",
      "[Train] step: 20500, loss: 0.20716, acc:0.95000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 20501, acc: 0.19100\n",
      "[Train] step: 20600, loss: 0.39174, acc:0.90000\n",
      "[Train] step: 20700, loss: 0.24771, acc:0.90000\n",
      "[Train] step: 20800, loss: 0.43894, acc:0.80000\n",
      "[Train] step: 20900, loss: 0.73284, acc:0.75000\n",
      "[Train] step: 21000, loss: 0.81095, acc:0.65000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 21001, acc: 0.21100\n",
      "[Train] step: 21100, loss: 0.45583, acc:0.85000\n",
      "[Train] step: 21200, loss: 0.63013, acc:0.75000\n",
      "[Train] step: 21300, loss: 0.58700, acc:0.70000\n",
      "[Train] step: 21400, loss: 0.90573, acc:0.75000\n",
      "[Train] step: 21500, loss: 0.80470, acc:0.70000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 21501, acc: 0.21550\n",
      "[Train] step: 21600, loss: 0.49998, acc:0.80000\n",
      "[Train] step: 21700, loss: 0.60573, acc:0.80000\n",
      "[Train] step: 21800, loss: 0.70458, acc:0.75000\n",
      "[Train] step: 21900, loss: 0.26695, acc:0.90000\n",
      "[Train] step: 22000, loss: 0.80388, acc:0.80000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 22001, acc: 0.18100\n",
      "[Train] step: 22100, loss: 0.48763, acc:0.85000\n",
      "[Train] step: 22200, loss: 0.25914, acc:0.85000\n",
      "[Train] step: 22300, loss: 0.90750, acc:0.70000\n",
      "[Train] step: 22400, loss: 0.43382, acc:0.85000\n",
      "[Train] step: 22500, loss: 0.55772, acc:0.75000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 22501, acc: 0.16950\n",
      "[Train] step: 22600, loss: 0.54468, acc:0.80000\n",
      "[Train] step: 22700, loss: 0.29853, acc:0.90000\n",
      "[Train] step: 22800, loss: 0.47388, acc:0.85000\n",
      "[Train] step: 22900, loss: 0.86900, acc:0.70000\n",
      "[Train] step: 23000, loss: 0.64563, acc:0.65000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 23001, acc: 0.18650\n",
      "[Train] step: 23100, loss: 0.25454, acc:0.90000\n",
      "[Train] step: 23200, loss: 0.11015, acc:0.95000\n",
      "[Train] step: 23300, loss: 0.73414, acc:0.70000\n",
      "[Train] step: 23400, loss: 0.33489, acc:0.75000\n",
      "[Train] step: 23500, loss: 0.34450, acc:0.85000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 23501, acc: 0.19150\n",
      "[Train] step: 23600, loss: 0.62040, acc:0.65000\n",
      "[Train] step: 23700, loss: 0.46506, acc:0.90000\n",
      "[Train] step: 23800, loss: 0.27344, acc:0.85000\n",
      "[Train] step: 23900, loss: 0.28978, acc:0.85000\n",
      "[Train] step: 24000, loss: 0.35869, acc:0.90000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 24001, acc: 0.19100\n",
      "[Train] step: 24100, loss: 0.57605, acc:0.85000\n",
      "[Train] step: 24200, loss: 0.23656, acc:0.90000\n",
      "[Train] step: 24300, loss: 0.47280, acc:0.75000\n",
      "[Train] step: 24400, loss: 0.17511, acc:0.90000\n",
      "[Train] step: 24500, loss: 0.51053, acc:0.80000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 24501, acc: 0.19750\n",
      "[Train] step: 24600, loss: 0.39274, acc:0.90000\n",
      "[Train] step: 24700, loss: 0.25318, acc:0.90000\n",
      "[Train] step: 24800, loss: 0.76759, acc:0.70000\n",
      "[Train] step: 24900, loss: 0.65392, acc:0.65000\n",
      "[Train] step: 25000, loss: 0.61911, acc:0.85000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 25001, acc: 0.19900\n",
      "[Train] step: 25100, loss: 0.30771, acc:0.90000\n",
      "[Train] step: 25200, loss: 0.68914, acc:0.75000\n",
      "[Train] step: 25300, loss: 0.46417, acc:0.80000\n",
      "[Train] step: 25400, loss: 0.56320, acc:0.80000\n",
      "[Train] step: 25500, loss: 0.48327, acc:0.80000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 25501, acc: 0.20750\n",
      "[Train] step: 25600, loss: 0.48124, acc:0.80000\n",
      "[Train] step: 25700, loss: 0.95263, acc:0.75000\n",
      "[Train] step: 25800, loss: 0.58031, acc:0.80000\n",
      "[Train] step: 25900, loss: 0.53466, acc:0.80000\n",
      "[Train] step: 26000, loss: 0.41435, acc:0.90000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 26001, acc: 0.20600\n",
      "[Train] step: 26100, loss: 0.63968, acc:0.75000\n",
      "[Train] step: 26200, loss: 0.42241, acc:0.85000\n",
      "[Train] step: 26300, loss: 0.50210, acc:0.85000\n",
      "[Train] step: 26400, loss: 0.49734, acc:0.80000\n",
      "[Train] step: 26500, loss: 0.78932, acc:0.70000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 26501, acc: 0.22250\n",
      "[Train] step: 26600, loss: 0.36447, acc:0.90000\n",
      "[Train] step: 26700, loss: 0.38846, acc:0.80000\n",
      "[Train] step: 26800, loss: 0.31875, acc:0.90000\n",
      "[Train] step: 26900, loss: 1.09562, acc:0.70000\n",
      "[Train] step: 27000, loss: 0.43626, acc:0.85000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 27001, acc: 0.19450\n",
      "[Train] step: 27100, loss: 0.58758, acc:0.85000\n",
      "[Train] step: 27200, loss: 0.65655, acc:0.80000\n",
      "[Train] step: 27300, loss: 0.41267, acc:0.90000\n",
      "[Train] step: 27400, loss: 0.57466, acc:0.80000\n",
      "[Train] step: 27500, loss: 0.39614, acc:0.90000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 27501, acc: 0.16800\n",
      "[Train] step: 27600, loss: 0.39154, acc:0.90000\n",
      "[Train] step: 27700, loss: 0.22174, acc:0.90000\n",
      "[Train] step: 27800, loss: 0.63978, acc:0.75000\n",
      "[Train] step: 27900, loss: 0.74207, acc:0.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] step: 28000, loss: 0.36336, acc:0.90000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 28001, acc: 0.19600\n",
      "[Train] step: 28100, loss: 0.49907, acc:0.90000\n",
      "[Train] step: 28200, loss: 0.31167, acc:0.95000\n",
      "[Train] step: 28300, loss: 0.77479, acc:0.75000\n",
      "[Train] step: 28400, loss: 0.68345, acc:0.75000\n",
      "[Train] step: 28500, loss: 0.64338, acc:0.70000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 28501, acc: 0.22700\n",
      "[Train] step: 28600, loss: 0.52438, acc:0.90000\n",
      "[Train] step: 28700, loss: 0.30442, acc:0.80000\n",
      "[Train] step: 28800, loss: 0.43614, acc:0.80000\n",
      "[Train] step: 28900, loss: 0.44348, acc:0.95000\n",
      "[Train] step: 29000, loss: 0.40654, acc:0.80000\n",
      "(10000, 3072) (10000,)\n",
      "[Test] Step: 29001, acc: 0.20400\n",
      "[Train] step: 29100, loss: 0.53740, acc:0.75000\n",
      "[Train] step: 29200, loss: 0.21555, acc:0.95000\n",
      "[Train] step: 29300, loss: 0.66803, acc:0.80000\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 20\n",
    "train_steps = 100000\n",
    "test_steps =100\n",
    "\n",
    "output_summary_every_steps = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # 2. 训练过程中将这些变量计算出来，输出到文件中\n",
    "    train_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(test_log_dir)\n",
    "    \n",
    "    fixed_test_batch_data, fixed_test_batch_labels = train_data.next_batch(batch_size)\n",
    "    for i in range(train_steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "        eval_ops = [loss, accuracy, train_op]\n",
    "        should_output_summary = ((i+1) % output_summary_every_steps == 0)\n",
    "        if should_output_summary:\n",
    "            eval_ops.append(merged_summary)\n",
    "        \n",
    "        eval_ops_results = sess.run(\n",
    "            eval_ops, \n",
    "            feed_dict={\n",
    "                x: batch_data,\n",
    "                y: batch_labels,\n",
    "                is_training: True})\n",
    "        loss_val, accu_val = eval_ops_results[0:2]\n",
    "        if should_output_summary:\n",
    "            train_summary_str = eval_ops_results[-1]\n",
    "            train_writer.add_summary(train_summary_str, i+1)\n",
    "            test_summary_str = sess.run([merged_summary_test],\n",
    "                                           feed_dict={\n",
    "                                               x: fixed_test_batch_data,\n",
    "                                               y: fixed_test_batch_labels,\n",
    "                                               is_training: False\n",
    "                                           })[0]\n",
    "            test_writer.add_summary(test_summary_str, i+1)\n",
    "        if i % 100 == 0:\n",
    "            print('[Train] step: %d, loss: %4.5f, acc:%4.5f'% (i, loss_val, accu_val)) \n",
    "        if i % 500 == 0:\n",
    "            test_data = CifarData(test_filenames, False)\n",
    "            \n",
    "            all_test_acc_val = []\n",
    "            \n",
    "            for j in range(test_steps):\n",
    "                test_batch_data, test_batch_labels = test_data.next_batch(batch_size)\n",
    "                test_acc_val = sess.run([accuracy],\n",
    "                                       feed_dict={\n",
    "                                           x: test_batch_data,\n",
    "                                           y: test_batch_labels,\n",
    "                                           is_training: False\n",
    "                                       })\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc = np.mean(all_test_acc_val)\n",
    "            print('[Test] Step: %d, acc: %4.5f'%(i+1, test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[Test] Step: %d, acc: %4.5f'%(i+1, test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 命令\n",
    "# tensorboard --logdir=train:'train',test:'test'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
